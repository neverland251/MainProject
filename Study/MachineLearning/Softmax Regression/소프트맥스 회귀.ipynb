{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame,Series\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.random.randn(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소프트맥스 함수 \n",
    "def softmax(data,params):\n",
    "    # 데이터 포인트와 파라미터를 내적한다. 이 때, 파라미터 행렬은 ((변수의 갯수)행 * (분류 클래스)열)의 행렬이다\n",
    "    # 예를들어, 변수가 4개이고, 클래스가 3개라면 파라미터 행렬은 4*3의 행렬이 된다.\n",
    "    score = data.dot(params)\n",
    "    # 각 클래스별 스코어값을 총합 스코어값으로 나누어준다. class = 1일때 해당 스코어벡터는 1열의 열벡터가 되는데\n",
    "    # 이를 1열, 2열, 3열을 모두 더한 총합 스코어 열벡터로 나누어 표준화를 해주는 작업이다.(keepdims는 행과 열이 교환되지 않도록 한다)\n",
    "    score = np.exp(score)/np.sum(np.exp(score),axis=1,keepdims=True)\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법 함수\n",
    "\n",
    "def gradient_descent(data,params,score,target,learning_rate,class_num):\n",
    "    # 0으로만 차있는 껍데기온 배열을 만든다.\n",
    "    gradient_vec = np.zeros((class_num,np.shape(data)[1]))\n",
    "    # 변수 의존성을 피하기 위해 copy()함수로 복사한다.\n",
    "    target_temp = target.copy()\n",
    "    for i in range(0,class_num):\n",
    "        # 자동 원핫 인코딩(1-1)\n",
    "        # 분류 클래스만큼 순회를 돈다. 해당 클래스의 차례(i가 해당 클래스)가 아닌 경우 일괄적으로 0으로 처리해준다\n",
    "        target_temp[target != i] = 0\n",
    "        # i가 해당 클래스인 경우 1로 켜준다.\n",
    "        target_temp[target == i] = 1\n",
    "        # 그래디언트 벡터의 오차 부분을 정의해준다.(1)\n",
    "        score[:,i] = score[:,i] - target_temp\n",
    "    # 모든 분류 클래스의 순회가 끝나면, 앞서 구한 오차 부분과 데이터를 내적한다. (2)\n",
    "    gradient_vec = data.T.dot(score) / np.shape(data)[0]\n",
    "    # (1)과 (2)로 그래디언트 벡터를 정의하였다. 이제 학습률에 그래디언트 벡터를 곱하여 뉴턴법을 실행한다.\n",
    "    params = params - learning_rate * gradient_vec\n",
    "    return (params,gradient_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#평가 함수(크로스 엔트로피)\n",
    "\n",
    "def scoring(data,score,target,class_num):\n",
    "    # 변수 의존성을 피하기 위해 copy()함수로 복사한다.\n",
    "    target_temp = target.copy()\n",
    "    # 변수 의존성을 피하기 위해 마찬가지로 copy()함수로 복사한다.\n",
    "    score_temp = score.copy()\n",
    "    for i in range(0,class_num):\n",
    "        #자동 원핫 인코딩(1-1 참조)\n",
    "        target_temp[target != i] = 0\n",
    "        target_temp[target == i] = 1\n",
    "        # 원핫 인코딩을 스위치처럼 활용한다. 로그 변환된 스코어 벡터는 만일 해당 데이터포인트가 순회하는 i에 해당하는 포인트가 아닐 경우\n",
    "        # 0으로 꺼진다. 즉 [a,a,0,0,0,0,0],[0,0,b,b,b,0,0],[0,0,0,0,c,c] 꼴로 출력된다.\n",
    "        score_temp[:,i] = np.log(score[:,i]) * target_temp\n",
    "    # log 변환하며 nan값이 필연적으로 생성된다. nan값을 무시하기 위해 nansum으로 더해주고, 이를 총 샘플수로 나누어 표준화한 값을\n",
    "    # 크로스 엔트로피 함수로 정의한다.\n",
    "    cross_entrophy = -np.nansum(score_temp)/np.shape(data)[0]\n",
    "    return(cross_entrophy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(loss,minimum_loss,b):\n",
    "    if loss > minimum_loss:\n",
    "        b += 1\n",
    "    if loss <= minimum_loss:\n",
    "        minimum_loss = loss.copy()\n",
    "        b = 0\n",
    "    return (b,minimum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression(data,target,iteration,learning_rate):\n",
    "    class_num = len(np.unique(target))\n",
    "    # 최초 파라미터를 랜덤 선출한다.\n",
    "    init_params = np.random.randn(np.shape(data)[1],len(np.unique(target)))\n",
    "    params = init_params.copy()\n",
    "    result = dict()\n",
    "    minimum_loss = 100000\n",
    "    b = 0\n",
    "    for i in range(0,iteration):\n",
    "        score = softmax(data,params)\n",
    "        loss = scoring(data,score,target,class_num)\n",
    "        print(i,loss)\n",
    "        # 만약 손실함수가 10회 이상 최저값을 경신하지 못할경우 순회를 중지한다\n",
    "        # 최저값을 경신할 경우 최저값을 갱신하고, b를 다시 0으로 초기화한다.\n",
    "        b, minimum_loss = early_stopping(loss,minimum_loss,b)\n",
    "        if b == 10:\n",
    "            print(\"early stopping activated\")\n",
    "            break\n",
    "        params,gradient_vec = gradient_descent(data,params,score,target,learning_rate,class_num)\n",
    "    result[\"score\"] = score\n",
    "    result[\"params\"] = params\n",
    "    result[\"gradient_vec\"] = gradient_vec\n",
    "    result[\"init_params\"] = init_params\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3818764148669216\n",
      "1 0.8613215322740342\n",
      "2 0.8234378941558895\n",
      "3 0.7927670876289302\n",
      "4 0.7677569320401676\n",
      "5 0.7487633159163992\n",
      "6 0.7362925822747709\n",
      "7 0.7332749405962883\n",
      "8 0.7396242813790034\n",
      "9 0.7564102497941549\n",
      "10 0.7773484112444303\n",
      "11 0.7888979457289715\n",
      "12 0.8006243460213669\n",
      "13 0.7851374188156641\n",
      "14 0.7920795635967574\n",
      "15 0.7641819934370244\n",
      "16 0.7760683484014977\n",
      "17 0.7436407940932338\n",
      "early stopping activated\n"
     ]
    }
   ],
   "source": [
    "result = softmax_regression(data,target,500,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스 확률 출력\n",
    "\n",
    "def class_proba(result,data):\n",
    "    proba_vec = np.exp(np.dot(result[\"params\"].T,data))/np.sum(np.exp(np.dot(result[\"params\"].T,data)))\n",
    "    return proba_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(class_proba(result,data[139]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.006, 0.941, 0.052])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_proba(result,data[85]).round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensorflow35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
