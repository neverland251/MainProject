{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Input,Dense,Flatten,Embedding,Permute,Dot,Reshape\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D,MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM,GRU\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화 파일을 불러온다.\n",
    "\n",
    "rawdata = pd.read_csv(\"토큰화.csv\",engine=\"python\",encoding = \"cp949\")\n",
    "\n",
    "morphs = list()\n",
    "\n",
    "for i in range(0,len(rawdata)):\n",
    "    morphs.append(list(rawdata.loc[i,:].dropna()))\n",
    "\n",
    "del morphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#색인 사전을 불러온다.\n",
    "\n",
    "morphsVectored = list()\n",
    "\n",
    "\n",
    "vocabulary = pd.read_csv(\"색인사전.csv\",engine=\"python\",encoding=\"cp949\")\n",
    "#del vocabulary[\"Unnamed: 0\"]\n",
    "\n",
    "vocabulary = vocabulary.to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98년생들에게 추천하는 맛있는 맥주 \\n\\n1. 브르클린 라거\\n2. 펑크 IPA\\...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>퇴근 후 아내가 차려준 주안상과 함께하는 듀벨~\\n\\n#beer #belgian #...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\n오랜만에 새로운 맥주 입고!\\n개인적으로 가장 좋아하는 #IPA #스컬핀 뉴시...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>스타ㅏㅏㅏ듀벨ㄹㄹ리이ㅣ이ㅣㅣ이ㅣㅣㅣ!!!!!!!!!~!~~!~!~!~!~!~!~!!...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>듀벨 트리플홉 2017이 입고됐습니다. 듀벨은 매년 기존 두 가지 홉 외에 하나를 ...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews   source target\n",
       "0  98년생들에게 추천하는 맛있는 맥주 \\n\\n1. 브르클린 라거\\n2. 펑크 IPA\\...  twitter     듀벨\n",
       "1  퇴근 후 아내가 차려준 주안상과 함께하는 듀벨~\\n\\n#beer #belgian #...  twitter     듀벨\n",
       "2  .\\n오랜만에 새로운 맥주 입고!\\n개인적으로 가장 좋아하는 #IPA #스컬핀 뉴시...  twitter     듀벨\n",
       "3  스타ㅏㅏㅏ듀벨ㄹㄹ리이ㅣ이ㅣㅣ이ㅣㅣㅣ!!!!!!!!!~!~~!~!~!~!~!~!~!!...  twitter     듀벨\n",
       "4  듀벨 트리플홉 2017이 입고됐습니다. 듀벨은 매년 기존 두 가지 홉 외에 하나를 ...  twitter     듀벨"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원본 데이터셋을 불러온다.\n",
    "\n",
    "a = pd.read_csv(\"beer.csv\",engine=\"python\",encoding=\"cp949\")\n",
    "#a = a[a[\"reviews\"].duplicated() == False]\n",
    "a = a.reset_index()\n",
    "del a[\"Unnamed: 0\"]\n",
    "del a[\"index\"]\n",
    "\n",
    "#타겟과 텍스트를 따로 저장한다.\n",
    "target = a[\"target\"]\n",
    "text= a[\"reviews\"]\n",
    "\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'morphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-b10a2d159526>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#로드한 토큰화 파일에서, 색인사전을 검색하여 토큰화 문장을 숫자 문장으로 바꿔준다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmorphs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtemporailyList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'morphs' is not defined"
     ]
    }
   ],
   "source": [
    "#로드한 토큰화 파일에서, 색인사전을 검색하여 토큰화 문장을 숫자 문장으로 바꿔준다.\n",
    "\n",
    "for i in morphs:\n",
    "    temporailyList = list()\n",
    "    for k in i:\n",
    "        #print(k)\n",
    "        try:\n",
    "            temporailyList.append(vocabulary[k])\n",
    "        except KeyError:\n",
    "            temporailyList.append(0)\n",
    "    morphsVectored.append(temporailyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화 문장에서 맥주 키워드(듀벨, 기네스, 카스 ,하이트 등등)을 제거한다.\n",
    "\n",
    "preprocessing_target_from_target = target.unique()\n",
    "\n",
    "for j,i in enumerate(morphs):\n",
    "    for k in preprocessing_target_from_target:\n",
    "        for l in np.where(np.array(i) == k)[0]:\n",
    "            try:\n",
    "                del morphs[j][l]\n",
    "            except IndexError : \n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_seq = sequence.pad_sequences(morphsVectored,maxlen = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['베를리너필스너' '필스너우르켈' '삿포로맥주' '아사히맥주' '칭따오맥주']\n"
     ]
    }
   ],
   "source": [
    "listed = list()\n",
    "error = list()\n",
    "\n",
    "for i in target:\n",
    "    try:\n",
    "        listed.append(vocabulary[i])\n",
    "    except : \n",
    "        error.append(i)\n",
    "print(pd.unique(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = re.compile(\"[맥주]+\")\n",
    "\n",
    "for i,j in target.items():\n",
    "    if j.endswith(\"맥주\"):\n",
    "        target[i] = names.sub(\"\",j)\n",
    "    else : pass\n",
    "\n",
    "names = re.compile(\"[베를리너]+\")\n",
    "\n",
    "for i,j in target.items():\n",
    "    if j.endswith(\"필스너\"):\n",
    "        target[i] = names.sub(\"\",j)\n",
    "    else : pass\n",
    "\n",
    "names = re.compile(\"[우르켈]+\")\n",
    "\n",
    "for i,j in target.items():\n",
    "    if j.endswith(\"우르켈\"):\n",
    "        target[i] = names.sub(\"\",j)\n",
    "    else : pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(vectorized_seq, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 타겟 변수 임베딩 버젼\n",
    "\n",
    "zero_pad_train = np.array([[0,vocabulary[i]] for i in y_train])\n",
    "zero_pad_test = np.array([[0,vocabulary[i]] for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#타겟 인코딩\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = le.inverse_transform(y_train)\n",
    "\n",
    "pd.unique(y_train)\n",
    "\n",
    "pd.unique(a)\n",
    "\n",
    "target_dict = {i:j for i,j in zip(pd.unique(y_train),pd.unique(a))}\n",
    "\n",
    "Series(target_dict).sort_index().to_csv(\"타겟.csv\",encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([[i,i]for i in y_train])\n",
    "y_test = np.array([[i,i]for i in y_test])\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련용 모듈\n",
    "\n",
    "## GRU 입력 전 사전처리 모듈(임베딩 -> 컨볼루션 -> 맥스풀링)\n",
    "\n",
    "inputs = layers.Input(shape=[50],name = \"Feed_Sentence\")\n",
    "\n",
    "embed = layers.Embedding(len(vocabulary)+1,128)\n",
    "\n",
    "embed_i = embed(inputs)\n",
    "model = layers.Dropout(0.2)(embed_i)\n",
    "model = layers.Conv1D(256,3,padding=\"valid\",activation=\"relu\",strides=1)(model)\n",
    "model = layers.MaxPooling1D(pool_size = 4)(model)\n",
    "\n",
    "inputs_d = layers.Input(shape = [2],name = \"Feed_ZeroPad_for_Decoder\")\n",
    "embed_o = embed(inputs_d)\n",
    "#embed_o = layers.Concatenate(axis=-1,name=\"embed_o_Doubled\")([embed_o,embed_o])\n",
    "\n",
    "\n",
    "#Bi-GRU 인코더 - 디코더 네트워크\n",
    "Encoder1 = layers.GRU(128,return_sequences = True,return_state = True,name=\"Encoder1\")\n",
    "Encoder2 = layers.GRU(128,return_sequences = True,return_state = True,go_backwards = True,name=\"Encoder2\")\n",
    "attention_matrix1,initial_1 = Encoder1(model)\n",
    "attention_matrix2,initial_2 = Encoder2(model)\n",
    "attention_matrix = layers.Concatenate(axis=-1,name = \"attention_matrix\")([attention_matrix1,attention_matrix2])\n",
    "\n",
    "\n",
    "Decoder1 = layers.GRU(128,return_sequences = True,name=\"Decoder1\")\n",
    "Decoder2 = layers.GRU(128,return_sequences = True,name=\"Decoder2\")\n",
    "Decoder1_output = Decoder1(embed_o,initial_state = initial_1)\n",
    "Decoder2_output = Decoder2(embed_o,initial_state = initial_2)\n",
    "Decoder_output = layers.Concatenate(axis=-1,name=\"Decoder_output\")([Decoder1_output,Decoder2_output])\n",
    "\n",
    "\n",
    "##어텐션 메커니즘 부분\n",
    "\n",
    "#normalize = True로 켠 상태에서, 코싸인 유사도를 구할 수 있도록 둘을 내적한다.  \n",
    "Cosine_similarity = layers.dot([Decoder_output,attention_matrix],axes = -1,normalize=True,name=\"Cosine_similarity\")\n",
    "\n",
    "#유사도 벡터를 softmax층에 통과시켜 총합이 1인 확률로 변환한다. 이를 attention_score로 명명한다.\n",
    "attention_score_layer = layers.Softmax(axis=-1,name=\"attention_score_from_Softmax\") \n",
    "attention_score = attention_score_layer(Cosine_similarity)\n",
    "\n",
    "#Softmax 변환된 attention_score를 최초의 attention_matrix와 각각 내적한다.\n",
    "#Transpose_attention_matrix = layers.Permute((2,1),name = \"Transpose_attention_matrix\")(attention_matrix)\n",
    "weighted_attention_matrix = layers.Lambda(lambda x: K.dot(x[0],x[1]),name=\"weighted_attention_matrix\")([attention_score,attention_matrix])\n",
    "#weighted_attention_matrix = layers.multiply([attention_score,Transpose_attention_matrix],name=\"weighted_attention_matrix\")\n",
    "\n",
    "#확률과 내적한 가중 attention_matrix의 열벡터를 모두 더해 1D 텐서인 context vector를 만들어준다.(1 * 256)\n",
    "context_vector = layers.Lambda(lambda x: K.sum(x, axis=2),name=\"Making_context_vector\")(weighted_attention_matrix)\n",
    "#context_vector_reshape = layers.Reshape((1,-1),name=\"Reshape_to_3D_tensor\")(context_vector)\n",
    "\n",
    "concat = layers.Concatenate(axis=-1,name = \"Concatenate_Decoder_O_and_Context_Vector\")([Decoder_output,context_vector])\n",
    "\n",
    "Feed_forward = layers.Dense(512,activation = \"tanh\",name=\"Feed_forward\")\n",
    "finally_output = Feed_forward(concat)\n",
    "\n",
    "predicts = layers.Dense(22,activation=\"softmax\")(finally_output)\n",
    "\n",
    "GRUs = Model(inputs = [inputs,inputs_d], outputs = [predicts])\n",
    "GRUs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 3)\n",
    "\n",
    "GRUs.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "GRUs.fit([X_train,zero_pad_train],y_train,epochs=5,batch_size = 64,validation_data = ([X_test,zero_pad_test],y_test),callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRUs.save(\"chatbot-attention_weight.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor36",
   "language": "python",
   "name": "tensorflow36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
