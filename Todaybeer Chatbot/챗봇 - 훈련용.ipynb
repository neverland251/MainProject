{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Input,Dense,Flatten,Embedding,Permute,Dot,Reshape\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D,MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM,GRU\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화 파일을 불러온다.\n",
    "\n",
    "rawdata = pd.read_csv(\"토큰화.csv\",engine=\"python\",encoding = \"cp949\")\n",
    "\n",
    "morphs = list()\n",
    "\n",
    "for i in range(0,len(rawdata)):\n",
    "    morphs.append(list(rawdata.loc[i,:].dropna()))\n",
    "\n",
    "del morphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#색인 사전을 불러온다.\n",
    "\n",
    "morphsVectored = list()\n",
    "\n",
    "\n",
    "vocabulary = pd.read_csv(\"색인사전.csv\",engine=\"python\",encoding=\"cp949\")\n",
    "#del vocabulary[\"Unnamed: 0\"]\n",
    "\n",
    "vocabulary = vocabulary.to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>98년생들에게 추천하는 맛있는 맥주 \\n\\n1. 브르클린 라거\\n2. 펑크 IPA\\...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>퇴근 후 아내가 차려준 주안상과 함께하는 듀벨~\\n\\n#beer #belgian #...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>.\\n오랜만에 새로운 맥주 입고!\\n개인적으로 가장 좋아하는 #IPA #스컬핀 뉴시...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>스타ㅏㅏㅏ듀벨ㄹㄹ리이ㅣ이ㅣㅣ이ㅣㅣㅣ!!!!!!!!!~!~~!~!~!~!~!~!~!!...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>듀벨 트리플홉 2017이 입고됐습니다. 듀벨은 매년 기존 두 가지 홉 외에 하나를 ...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>듀벨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            reviews   source  \\\n",
       "0           0  98년생들에게 추천하는 맛있는 맥주 \\n\\n1. 브르클린 라거\\n2. 펑크 IPA\\...  twitter   \n",
       "1           1  퇴근 후 아내가 차려준 주안상과 함께하는 듀벨~\\n\\n#beer #belgian #...  twitter   \n",
       "2           2  .\\n오랜만에 새로운 맥주 입고!\\n개인적으로 가장 좋아하는 #IPA #스컬핀 뉴시...  twitter   \n",
       "3           3  스타ㅏㅏㅏ듀벨ㄹㄹ리이ㅣ이ㅣㅣ이ㅣㅣㅣ!!!!!!!!!~!~~!~!~!~!~!~!~!!...  twitter   \n",
       "4           4  듀벨 트리플홉 2017이 입고됐습니다. 듀벨은 매년 기존 두 가지 홉 외에 하나를 ...  twitter   \n",
       "\n",
       "  target  \n",
       "0     듀벨  \n",
       "1     듀벨  \n",
       "2     듀벨  \n",
       "3     듀벨  \n",
       "4     듀벨  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#원본 데이터셋을 불러온다.\n",
    "\n",
    "a = pd.read_csv(\"beer.csv\",engine=\"python\",encoding=\"cp949\")\n",
    "#a = a[a[\"reviews\"].duplicated() == False]\n",
    "a = a.reset_index()\n",
    "del a[\"index\"]\n",
    "\n",
    "#타겟과 텍스트를 따로 저장한다.\n",
    "target = a[\"target\"]\n",
    "text= a[\"reviews\"]\n",
    "\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67316"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#토큰화 문장에서 맥주 키워드(듀벨, 기네스, 카스 ,하이트 등등)을 제거한다.\n",
    "\n",
    "preprocessing_target_from_target = target.unique()\n",
    "\n",
    "for j,i in enumerate(morphs):\n",
    "    for k in preprocessing_target_from_target:\n",
    "        for l in np.where(np.array(i) == k)[0]:\n",
    "            try:\n",
    "                del morphs[j][l]\n",
    "            except IndexError : \n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67316"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(morphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#로드한 토큰화 파일에서, 색인사전을 검색하여 토큰화 문장을 숫자 문장으로 바꿔준다.\n",
    "\n",
    "for i in morphs:\n",
    "    temporailyList = list()\n",
    "    for k in i:\n",
    "        #print(k)\n",
    "        try:\n",
    "            temporailyList.append(vocabulary[k])\n",
    "        except KeyError:\n",
    "            temporailyList.append(0)\n",
    "    morphsVectored.append(temporailyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67316"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(morphsVectored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_seq = sequence.pad_sequences(morphsVectored,maxlen = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(vectorized_seq, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_pad_train = None\n",
    "zero_pad_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 타겟 변수 임베딩 버젼\n",
    "\n",
    "zero_pad_train = np.array([[0,0,0,0,vocabulary[i]] for i in y_train])\n",
    "zero_pad_test = np.array([[0,0,0,0,0] for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#타겟 인코딩\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = le.inverse_transform(y_train)\n",
    "\n",
    "pd.unique(y_train)\n",
    "\n",
    "pd.unique(a)\n",
    "\n",
    "target_dict = {i:j for i,j in zip(pd.unique(y_train),pd.unique(a))}\n",
    "\n",
    "Series(target_dict).sort_index().to_csv(\"타겟.csv\",encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([[i,i,i,i,i] for i in y_train])\n",
    "y_test = np.array([[i,i,i,i,i] for i in y_test])\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_14:0' shape=(128,) dtype=float32>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decoder1.output[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Feed_ZeroPad_for_Decoder (Input (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Feed_Sentence (InputLayer)      (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        multiple             6537344     Feed_Sentence[0][0]              \n",
      "                                                                 Feed_ZeroPad_for_Decoder[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 50, 128)      0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 48, 256)      98560       dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 12, 256)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder1 (GRU)                  [(None, 12, 128), (N 147840      max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Encoder2 (GRU)                  [(None, 12, 128), (N 147840      max_pooling1d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Decoder1 (GRU)                  (None, 2, 128)       98688       embedding_12[1][0]               \n",
      "                                                                 Encoder1[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder2 (GRU)                  (None, 2, 128)       98688       embedding_12[1][0]               \n",
      "                                                                 Encoder2[0][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_output (Concatenate)    (None, 2, 256)       0           Decoder1[0][0]                   \n",
      "                                                                 Decoder2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_matrix (Concatenate)  (None, 12, 256)      0           Encoder1[0][0]                   \n",
      "                                                                 Encoder2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Cosine_similarity (Dot)         (None, 2, 12)        0           Decoder_output[0][0]             \n",
      "                                                                 attention_matrix[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention_score_from_Softmax (S (None, 2, 12)        0           Cosine_similarity[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "weighted_attention_matrix (Lamb (None, 2, None, 256) 0           attention_score_from_Softmax[0][0\n",
      "                                                                 attention_matrix[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Making_context_vector (Lambda)  (None, 2, 256)       0           weighted_attention_matrix[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Concatenate_Decoder_O_and_Conte (None, 2, 512)       0           Decoder_output[0][0]             \n",
      "                                                                 Making_context_vector[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Feed_forward (Dense)            (None, 2, 512)       262656      Concatenate_Decoder_O_and_Context\n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 2, 21)        10773       Feed_forward[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,402,389\n",
      "Trainable params: 7,402,389\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#훈련용 모듈\n",
    "\n",
    "## GRU 입력 전 사전처리 모듈(임베딩 -> 컨볼루션 -> 맥스풀링)\n",
    "\n",
    "inputs = layers.Input(shape=[50],name = \"Feed_Sentence\")\n",
    "\n",
    "embed = layers.Embedding(len(vocabulary)+1,128)\n",
    "\n",
    "embed_i = embed(inputs)\n",
    "model = layers.Dropout(0.2)(embed_i)\n",
    "model = layers.Conv1D(256,3,padding=\"valid\",activation=\"relu\",strides=1)(model)\n",
    "model = layers.MaxPooling1D(pool_size = 4)(model)\n",
    "\n",
    "inputs_d = layers.Input(shape = [2],name = \"Feed_ZeroPad_for_Decoder\")\n",
    "embed_o = embed(inputs_d)\n",
    "#embed_o = layers.Concatenate(axis=-1,name=\"embed_o_Doubled\")([embed_o,embed_o])\n",
    "\n",
    "\n",
    "#Bi-GRU 인코더 - 디코더 네트워크\n",
    "Encoder1 = layers.GRU(128,return_sequences = True,return_state = True,name=\"Encoder1\")\n",
    "Encoder2 = layers.GRU(128,return_sequences = True,return_state = True,go_backwards = True,name=\"Encoder2\")\n",
    "attention_matrix1,initial_1 = Encoder1(model)\n",
    "attention_matrix2,initial_2 = Encoder2(model)\n",
    "attention_matrix = layers.Concatenate(axis=-1,name = \"attention_matrix\")([attention_matrix1,attention_matrix2])\n",
    "\n",
    "\n",
    "Decoder1 = layers.GRU(128,return_sequences = True,name=\"Decoder1\")\n",
    "Decoder2 = layers.GRU(128,return_sequences = True,name=\"Decoder2\")\n",
    "Decoder1_output = Decoder1(embed_o,initial_state = initial_1)\n",
    "Decoder2_output = Decoder2(embed_o,initial_state = initial_2)\n",
    "Decoder_output = layers.Concatenate(axis=-1,name=\"Decoder_output\")([Decoder1_output,Decoder2_output])\n",
    "\n",
    "\n",
    "##어텐션 메커니즘 부분\n",
    "\n",
    "#normalize = True로 켠 상태에서, 코싸인 유사도를 구할 수 있도록 둘을 내적한다.  \n",
    "Cosine_similarity = layers.dot([Decoder_output,attention_matrix],axes = -1,normalize=True,name=\"Cosine_similarity\")\n",
    "\n",
    "#유사도 벡터를 softmax층에 통과시켜 총합이 1인 확률로 변환한다. 이를 attention_score로 명명한다.\n",
    "attention_score_layer = layers.Softmax(axis=-1,name=\"attention_score_from_Softmax\") \n",
    "attention_score = attention_score_layer(Cosine_similarity)\n",
    "\n",
    "#Softmax 변환된 attention_score를 최초의 attention_matrix와 각각 내적한다.\n",
    "#Transpose_attention_matrix = layers.Permute((2,1),name = \"Transpose_attention_matrix\")(attention_matrix)\n",
    "weighted_attention_matrix = layers.Lambda(lambda x: K.dot(x[0],x[1]),name=\"weighted_attention_matrix\")([attention_score,attention_matrix])\n",
    "#weighted_attention_matrix = layers.multiply([attention_score,Transpose_attention_matrix],name=\"weighted_attention_matrix\")\n",
    "\n",
    "#확률과 내적한 가중 attention_matrix의 열벡터를 모두 더해 1D 텐서인 context vector를 만들어준다.(1 * 256)\n",
    "context_vector = layers.Lambda(lambda x: K.sum(x, axis=2),name=\"Making_context_vector\")(weighted_attention_matrix)\n",
    "#context_vector_reshape = layers.Reshape((1,-1),name=\"Reshape_to_3D_tensor\")(context_vector)\n",
    "\n",
    "concat = layers.Concatenate(axis=-1,name = \"Concatenate_Decoder_O_and_Context_Vector\")([Decoder_output,context_vector])\n",
    "\n",
    "Feed_forward = layers.Dense(512,activation = \"tanh\",name=\"Feed_forward\")\n",
    "finally_output = Feed_forward(concat)\n",
    "\n",
    "predicts = layers.Dense(21,activation=\"softmax\")(finally_output)\n",
    "\n",
    "GRUs = Model(inputs = [inputs,inputs_d], outputs = [predicts])\n",
    "GRUs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50487 samples, validate on 16829 samples\n",
      "Epoch 1/10\n",
      "50487/50487 [==============================] - 151s 3ms/step - loss: 2.0734 - acc: 0.3455 - val_loss: 2.0355 - val_acc: 0.3744\n",
      "Epoch 2/10\n",
      "50487/50487 [==============================] - 146s 3ms/step - loss: 1.4160 - acc: 0.5547 - val_loss: 1.8504 - val_acc: 0.4267\n",
      "Epoch 3/10\n",
      "50487/50487 [==============================] - 146s 3ms/step - loss: 1.0500 - acc: 0.6654 - val_loss: 1.9453 - val_acc: 0.4458\n",
      "Epoch 4/10\n",
      "50487/50487 [==============================] - 146s 3ms/step - loss: 0.7548 - acc: 0.7588 - val_loss: 2.2557 - val_acc: 0.4393\n",
      "Epoch 5/10\n",
      "50487/50487 [==============================] - 146s 3ms/step - loss: 0.5616 - acc: 0.8189 - val_loss: 2.5839 - val_acc: 0.4307\n",
      "Epoch 6/10\n",
      "50487/50487 [==============================] - 147s 3ms/step - loss: 0.4449 - acc: 0.8574 - val_loss: 2.7913 - val_acc: 0.4268\n",
      "Epoch 7/10\n",
      "50487/50487 [==============================] - 146s 3ms/step - loss: 0.3716 - acc: 0.8806 - val_loss: 3.1345 - val_acc: 0.4234\n",
      "Epoch 8/10\n",
      "50487/50487 [==============================] - 146s 3ms/step - loss: 0.3216 - acc: 0.8954 - val_loss: 3.3486 - val_acc: 0.4214\n",
      "Epoch 9/10\n",
      "50487/50487 [==============================] - 146s 3ms/step - loss: 0.2897 - acc: 0.9069 - val_loss: 3.4390 - val_acc: 0.4157\n",
      "Epoch 10/10\n",
      "50487/50487 [==============================] - 146s 3ms/step - loss: 0.2581 - acc: 0.9171 - val_loss: 3.5287 - val_acc: 0.4170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d498af5be0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 20)\n",
    "\n",
    "GRUs.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "GRUs.fit([X_train,zero_pad_train],y_train,epochs=10,batch_size = 64,validation_data = ([X_test,zero_pad_test],y_test),callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer Decoder1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder1_6/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer Decoder2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder2_6/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "GRUs.save(\"chatbot-attention_weight_one\n",
    "          .h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor36",
   "language": "python",
   "name": "tensorflow36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
