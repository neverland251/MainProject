{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchWindowException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains as AC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "from pandas import Series,DataFrame\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import csv\n",
    "\n",
    "from time import sleep\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Input,Dense,Flatten,Embedding,Permute,Dot,Reshape\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D,MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM,GRU\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import function\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "import copy\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "Okt = Okt()\n",
    "Kom = Komoran()\n",
    "Kkma = Kkma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비 및 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 데이터 LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "list_dir = os.listdir(\"C:/Users/23/Desktop/Project/코로나/데이터/\")\n",
    "file_list = np.array(list_dir)[np.where(DataFrame(list(Series(list_dir).apply(lambda x : x.split(\".\"))))[[3]] == \"txt\")[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rawdata = DataFrame()\n",
    "\n",
    "for i in file_list:\n",
    "    texts = Series()\n",
    "    with open(\"C:/Users/23/Desktop/Project/코로나/데이터/\" + str(i) ,encoding = \"utf-8\") as text:\n",
    "        for line in text:\n",
    "            texts = pd.concat([texts, Series(line).str.rsplit(\"\\t\")])\n",
    "            columns = [\"DATE\", \"PRESS\", \"TITLE\", \"TEXT\"]\n",
    "            col_length = DataFrame(list(pd.concat([texts, Series(line).str.rsplit(\"\\t\")]))).shape[1]\n",
    "            columns.extend([i for i in range(0 , (col_length - 4))])\n",
    "        texts = DataFrame(list(texts.reset_index(drop = True)),columns = columns).iloc[:,0:4]\n",
    "        rawdata = pd.concat([rawdata, texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = rawdata[\"TEXT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = rawdata[\"PRESS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts_for_vocabulary = pd.concat([text,target])\n",
    "texts_for_vocuabulary= text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "length_1 = DataFrame()\n",
    "\n",
    "for i,j in enumerate(np.array(text)):\n",
    "    good = DataFrame([len(j)],index=[i])\n",
    "    length_1 = pd.concat([length_1,good],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAASMElEQVR4nO3df6xc513n8fdnbZqWuGmdDb2y7GjtIgPrNKI0V9lCV9W1stqYFq2DRCRDAHcVZAlSCKvyh7P8Uf6xNrtSkEpLWHlJtYZGvZhQZGtLgMhwhZBosnFJ6zjGjUtC6sTYsGlDXFVpHb78MSfV7O29yZ2Z+2vmeb+k0Zx55pzzPN97rj9z7nNmxqkqJElt+FdrPQBJ0uox9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1pQEmuTfKHSb6e5O+S/NRaj0laqo1rPQBpDP0m8E1gCng38NkkX6iq02s7LOmNxU/kSkuX5Grgq8C7qupLXdvvAs9X1cE1HZy0BE7vSIP5PuDV1wK/8wXghjUajzQQQ18azCbgpXltLwFvXYOxSAMz9KXBXAaumdd2DfDyGoxFGpihLw3mS8DGJDv72n4Q8CKuxoIXcqUBJZkFCvg5eu/e+SPgR3z3jsaBZ/rS4H4BeAtwCfg08PMGvsaFZ/qS1BDP9CWpIYa+JDXE0Jekhhj6ktSQdf+Fa9ddd11t3759qG2//vWvc/XVVy/vgNaJSa1tUusCaxtX41rbyZMn/7Gqvmd++7oP/e3bt/P4448Pte3c3BwzMzPLO6B1YlJrm9S6wNrG1bjWluTvFmp3ekeSGvKGoZ/kk0kuJXmyr+3aJI8kebq739z33D1JziU5m+TWvvabkpzqnvuNJFn+ciRJr2cpZ/r/G9gzr+0gcKKqdgInusck2QXso/c1s3uA+5Ns6Lb5LeAAsLO7zd+nJGmFvWHoV9VfAC/Oa94LHOmWjwC39bXPVtUrVfUMcA64OckW4Jqq+qvqfQT4d/q2kSStkmEv5E5V1QWAqrqQ5B1d+1bgc33rne/avtUtz29fUJID9P4qYGpqirm5uaEGefny5aG3Xe8mtbZJrQusbVxNWm3L/e6dhebp63XaF1RVh4HDANPT0zXslfNxveq+FJNa26TWBdY2riattmHfvXOxm7Khu7/UtZ8Hru9bbxvwQte+bYF2SdIqGjb0jwP7u+X9wLG+9n1Jrkqyg94F28e6qaCXk7y3e9fOz/ZtI0laJW84vZPk08AMcF2S88BHgXuBo0nuBJ4DbgeoqtNJjgJPAVeAu6rq1W5XP0/vnUBvAR7ubpKkVfSGoV9VP7nIU7cssv4h4NAC7Y8D7xpodCM69fxLfOjgZ1ezSwCevfeDq96nJC2Fn8iVpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1JCRQj/Jf0lyOsmTST6d5M1Jrk3ySJKnu/vNfevfk+RckrNJbh19+JKkQQwd+km2Ar8ETFfVu4ANwD7gIHCiqnYCJ7rHJNnVPX8DsAe4P8mG0YYvSRrEqNM7G4G3JNkIfDfwArAXONI9fwS4rVveC8xW1StV9QxwDrh5xP4lSQNIVQ2/cXI3cAj4BvCnVXVHkq9V1dv71vlqVW1O8gngc1X1qa79AeDhqnpogf0eAA4ATE1N3TQ7OzvU+C69+BIXvzHUpiO5cevbVryPy5cvs2nTphXvZ7VNal1gbeNqXGvbvXv3yaqant++cdgddnP1e4EdwNeA30/y06+3yQJtC77iVNVh4DDA9PR0zczMDDXGjz94jPtODV3i0J69Y2bF+5ibm2PYn8t6Nql1gbWNq0mrbZTpnf8APFNV/1BV3wI+A/wIcDHJFoDu/lK3/nng+r7tt9GbDpIkrZJRQv854L1JvjtJgFuAM8BxYH+3zn7gWLd8HNiX5KokO4CdwGMj9C9JGtDQcx9V9WiSh4DPA1eAv6Y3JbMJOJrkTnovDLd3659OchR4qlv/rqp6dcTxS5IGMNKEd1V9FPjovOZX6J31L7T+IXoXfiVJa8BP5EpSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkNGCv0kb0/yUJK/SXImyQ8nuTbJI0me7u43961/T5JzSc4muXX04UuSBjHqmf7HgD+uqh8AfhA4AxwETlTVTuBE95gku4B9wA3AHuD+JBtG7F+SNIChQz/JNcD7gQcAquqbVfU1YC9wpFvtCHBbt7wXmK2qV6rqGeAccPOw/UuSBpeqGm7D5N3AYeApemf5J4G7geer6u196321qjYn+QTwuar6VNf+APBwVT20wL4PAAcApqambpqdnR1qjJdefImL3xhq05HcuPVtK97H5cuX2bRp04r3s9omtS6wtnE1rrXt3r37ZFVNz2/fOMI+NwLvAX6xqh5N8jG6qZxFZIG2BV9xquowvRcUpqena2ZmZqgBfvzBY9x3apQSh/PsHTMr3sfc3BzD/lzWs0mtC6xtXE1abaPM6Z8HzlfVo93jh+i9CFxMsgWgu7/Ut/71fdtvA14YoX9J0oCGDv2q+nvgK0m+v2u6hd5Uz3Fgf9e2HzjWLR8H9iW5KskOYCfw2LD9S5IGN+rcxy8CDyZ5E/C3wH+m90JyNMmdwHPA7QBVdTrJUXovDFeAu6rq1RH7lyQNYKTQr6ongO+4UEDvrH+h9Q8Bh0bpU5I0PD+RK0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDRk59JNsSPLXSf5P9/jaJI8kebq739y37j1JziU5m+TWUfuWJA1mOc707wbO9D0+CJyoqp3Aie4xSXYB+4AbgD3A/Uk2LEP/kqQlGin0k2wDPgj8dl/zXuBIt3wEuK2vfbaqXqmqZ4BzwM2j9C9JGkyqaviNk4eA/wa8FfiVqvqxJF+rqrf3rfPVqtqc5BPA56rqU137A8DDVfXQAvs9ABwAmJqauml2dnao8V168SUufmOoTUdy49a3rXgfly9fZtOmTSvez2qb1LrA2sbVuNa2e/fuk1U1Pb9947A7TPJjwKWqOplkZimbLNC24CtOVR0GDgNMT0/XzMxSdv+dPv7gMe47NXSJQ3v2jpkV72Nubo5hfy7r2aTWBdY2riattlES8X3Af0ryAeDNwDVJPgVcTLKlqi4k2QJc6tY/D1zft/024IUR+pckDWjoOf2quqeqtlXVdnoXaP+sqn4aOA7s71bbDxzrlo8D+5JclWQHsBN4bOiRS5IGthJzH/cCR5PcCTwH3A5QVaeTHAWeAq4Ad1XVqyvQvyRpEcsS+lU1B8x1y/8PuGWR9Q4Bh5ajT0nS4PxEriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWTo0E9yfZI/T3Imyekkd3ft1yZ5JMnT3f3mvm3uSXIuydkkty5HAZKkpRvlTP8K8JGq+rfAe4G7kuwCDgInqmoncKJ7TPfcPuAGYA9wf5INowxekjSYoUO/qi5U1ee75ZeBM8BWYC9wpFvtCHBbt7wXmK2qV6rqGeAccPOw/UuSBrcsc/pJtgM/BDwKTFXVBei9MADv6FbbCnylb7PzXZskaZVsHHUHSTYBfwD8clX9U5JFV12grRbZ5wHgAMDU1BRzc3NDjW3qLfCRG68Mte0ohh3vIC5fvrwq/ay2Sa0LrG1cTVptI4V+ku+iF/gPVtVnuuaLSbZU1YUkW4BLXft54Pq+zbcBLyy036o6DBwGmJ6erpmZmaHG9/EHj3HfqZFf1wb27B0zK97H3Nwcw/5c1rNJrQusbVxNWm2jvHsnwAPAmar69b6njgP7u+X9wLG+9n1JrkqyA9gJPDZs/5KkwY1yGvw+4GeAU0me6Nr+K3AvcDTJncBzwO0AVXU6yVHgKXrv/Lmrql4doX9J0oCGDv2q+ksWnqcHuGWRbQ4Bh4btU5I0Gj+RK0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGrP6XzTdg+8HPrngfH7nxCh9aoJ9n7/3givctaXx5pi9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1Jaoj/icqEWY3/wGUh/uct0njwTF+SGmLoS1JDDH1JaoihL0kNWfULuUn2AB8DNgC/XVX3rvYYtPyW6wLyR268wocG3JcXkaWlW9Uz/SQbgN8EfhTYBfxkkl2rOQZJatlqn+nfDJyrqr8FSDIL7AWeWuVxaIL4NlWtpFPPvzTwX5/LYaV+v1JVK7LjBTtLfgLYU1U/1z3+GeDfVdWH5613ADjQPfx+4OyQXV4H/OOQ2653k1rbpNYF1jauxrW2f1NV3zO/cbXP9LNA23e86lTVYeDwyJ0lj1fV9Kj7WY8mtbZJrQusbVxNWm2r/e6d88D1fY+3AS+s8hgkqVmrHfr/F9iZZEeSNwH7gOOrPAZJataqTu9U1ZUkHwb+hN5bNj9ZVadXsMuRp4jWsUmtbVLrAmsbVxNV26peyJUkrS0/kStJDTH0JakhExn6SfYkOZvkXJKDaz2epUrybJJTSZ5I8njXdm2SR5I83d1v7lv/nq7Gs0lu7Wu/qdvPuSS/kWSht8qudC2fTHIpyZN9bctWS5Krkvxe1/5oku1rWNevJXm+O25PJPnAuNXV9X19kj9PcibJ6SR3d+2TcNwWq20ijt1AqmqibvQuEH8ZeCfwJuALwK61HtcSx/4scN28tv8BHOyWDwL/vVve1dV2FbCjq3lD99xjwA/T+1zEw8CPrkEt7wfeAzy5ErUAvwD8z255H/B7a1jXrwG/ssC6Y1NX198W4D3d8luBL3U1TMJxW6y2iTh2g9wm8Uz/21/1UFXfBF77qodxtRc40i0fAW7ra5+tqleq6hngHHBzki3ANVX1V9X77fudvm1WTVX9BfDivOblrKV/Xw8Bt6zGXzSL1LWYsakLoKouVNXnu+WXgTPAVibjuC1W22LGprZBTWLobwW+0vf4PK9/cNeTAv40ycn0vooCYKqqLkDvFxd4R9e+WJ1bu+X57evBctby7W2q6grwEvCvV2zkb+zDSb7YTf+8Nv0xtnV1UxM/BDzKhB23ebXBhB27NzKJob+kr3pYp95XVe+h9y2kdyV5/+usu1id41j/MLWspzp/C/he4N3ABeC+rn0s60qyCfgD4Jer6p9eb9UF2tZ1fQvUNlHHbikmMfTH9qsequqF7v4S8If0pqoudn9S0t1f6lZfrM7z3fL89vVgOWv59jZJNgJvY+nTLsuqqi5W1atV9c/A/6J33P6/MXbWfV1JvoteKD5YVZ/pmifiuC1U2yQdu6WaxNAfy696SHJ1kre+tgz8R+BJemPf3622HzjWLR8H9nXvGNgB7AQe6/78fjnJe7v5xJ/t22atLWct/fv6CeDPujnWVfdaIHZ+nN5xgzGrqxvLA8CZqvr1vqfG/rgtVtukHLuBrPWV5JW4AR+gd3X+y8CvrvV4ljjmd9J7t8AXgNOvjZvenOAJ4Onu/tq+bX61q/Esfe/QAabp/fJ+GfgE3SevV7meT9P7c/lb9M6A7lzOWoA3A79P7wLbY8A717Cu3wVOAV+k9w9/y7jV1fX97+lNR3wReKK7fWBCjttitU3EsRvk5tcwSFJDJnF6R5K0CENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNeRfAHcKvH50oFl0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = length_1.hist()\n",
    "plt.show(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "\n",
    "length_1 = DataFrame()\n",
    "\n",
    "for i,j in enumerate(np.array(morphsVectored[(int(len(morphsVectored) / 2)) + 1 : len(morphsVectored)])):\n",
    "    good = DataFrame([len(j)],index=[i])\n",
    "    length_1 = pd.concat([length_1,good],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = length_1.hist()\n",
    "plt.show(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_tokenizer(text):\n",
    "    return Kom.nouns(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 색인사전 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = texts_for_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "있는 색인사전을 쓸까요(0), 아님 신규로 만들까요(1)1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "## CountVectorizer로 문자 : 숫자로 이루어진 색인 사전을 만든다.\n",
    "## 이미 만들어진 색인사전을 이용해도 됩니다.\n",
    "\n",
    "zero_or_one = input(\"있는 색인사전을 쓸까요(0), 아님 신규로 만들까요(1)\")\n",
    "\n",
    "if zero_or_one == \"1\":\n",
    "    vect = CountVectorizer(tokenizer = vect_tokenizer ,min_df = 1, analyzer = \"word\")\n",
    "    vect.fit(text)\n",
    "    vocabulary = vect.vocabulary_\n",
    "    vocabulary['CLS'] = len(vocabulary) + 1\n",
    "    vocabulary['EOS'] = len(vocabulary) + 1\n",
    "    DataFrame([vect.vocabulary_]).to_csv(\"색인사전.csv\",encoding=\"utf-8\",index=False)\n",
    "if zero_or_one == \"0\":\n",
    "    vocabulary = pd.read_csv(\"색인사전.csv\",engine=\"python\",encoding=\"utf-8\")\n",
    "    vocabulary = vocabulary.to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 문장  토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "있는 벡터화 쓸까요(0), 아님 신규로 만들까요(1)1\n"
     ]
    }
   ],
   "source": [
    "zero_or_one = input(\"있는 벡터화 쓸까요(0), 아님 신규로 만들까요(1)\")\n",
    "\n",
    "morphsVectored = list()\n",
    "\n",
    "if zero_or_one == \"1\":\n",
    "\n",
    "    ## 다시 형태소 분석기를 돌려서, text의 문장을 형태소로 분해하여 morphs에 담는다.\n",
    "    ## 시간이 오래 걸립니다. 5분정도\n",
    "\n",
    "    morphs = list()\n",
    "\n",
    "    for i in text:\n",
    "        temp = Kom.nouns(i)\n",
    "        temp.insert(0,'CLS')\n",
    "        temp.append(\"EOS\")\n",
    "        morphs.append(temp)\n",
    "        \n",
    "    for i in morphs:\n",
    "        temporailyList = list()\n",
    "        for k in i:\n",
    "            #print(k)\n",
    "            try:\n",
    "                temporailyList.append(vocabulary[k])\n",
    "            except KeyError:\n",
    "                temporailyList.append(0)\n",
    "        morphsVectored.append(temporailyList)\n",
    "\n",
    "    DataFrame(morphsVectored).to_csv(\"벡터화.csv\",index=False,encoding=\"utf-8\")\n",
    "\n",
    "if zero_or_one == \"0\" :\n",
    "    morphsVectored = list()\n",
    "    rawdata = pd.read_csv(\"벡터화.csv\",engine=\"python\",encoding =\" utf-8\")\n",
    "\n",
    "    for i in range(0,len(rawdata)):\n",
    "        morphsVectored.append(list(rawdata.loc[i,:].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text classification\n",
    "target = pd.get_dummies(DataFrame(target))\n",
    "\n",
    "input_1 = sequence.pad_sequences(morphsVectored ,maxlen = 70)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_1, target ,test_size=0.25)\n",
    "#y_train = np_utils.to_categorical(y_train)\n",
    "#y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generating\n",
    "\n",
    "input_1 = sequence.pad_sequences(morphsVectored[1 : (int(len(morphsVectored) / 2))],maxlen = 70)\n",
    "\n",
    "input_2 = sequence.pad_sequences(morphsVectored[(int(len(morphsVectored) / 2)) + 1 : len(morphsVectored)],maxlen = 20)\n",
    "\n",
    "X_train_1,X_val_1,X_train_2,X_val_2 = train_test_split(input_1, input_2 ,test_size=0.25)\n",
    "#y_train = np_utils.to_categorical(y_train)\n",
    "#y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "dummies_temp = DataFrame(columns = [i for i in range(1,len(vocabulary) + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positioning Encoding 생성 함수\n",
    "\n",
    "def get_timing_signal_1d(length,\n",
    "                         channels,\n",
    "                         min_timescale=1.0,\n",
    "                         max_timescale=1.0e4,\n",
    "                         start_index=0):\n",
    "    position = tf.cast(tf.range(length) + start_index,dtype=tf.float32)\n",
    "    num_timescales = channels // 2\n",
    "    log_timescale_increment = (\n",
    "        math.log(float(max_timescale) / float(min_timescale)) /\n",
    "        (tf.cast(num_timescales, dtype=tf.float32) - 1))\n",
    "    inv_timescales = min_timescale * tf.exp(\n",
    "        tf.cast(tf.range(num_timescales),dtype=tf.float32) * -log_timescale_increment)\n",
    "    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "    signal = tf.pad(signal, [[0, 0], [0, tf.math.floormod(channels, 2)]])\n",
    "    signal = tf.reshape(signal, [1, length, channels])\n",
    "    return signal\n",
    "\n",
    "# 패딩 마스크 생성 함수\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "#미리보기 마스크 생성 함수\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    # 단어 순서에 따라 상삼각행렬을 만들어준다.\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_classification\n",
    "\n",
    "multi_head_num = 8\n",
    "encoder_num = 5\n",
    "decoder_num = 5\n",
    "vocab_size = len(vocabulary) + 1\n",
    "target_size = target.shape[1]\n",
    "embedding_dim = 300\n",
    "dense_dim = 100\n",
    "\n",
    "sen_len_1 = input_1[0].shape[0]\n",
    "sen_len_2 = input_1[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_generating\n",
    "\n",
    "multi_head_num = 8\n",
    "encoder_num = 5\n",
    "decoder_num = 5\n",
    "vocab_size = len(vocabulary) + 1\n",
    "target_size = len(vocabulary) + 1\n",
    "embedding_dim = 300\n",
    "dense_dim = 100\n",
    "\n",
    "sen_len_1 = input_1[0].shape[0]\n",
    "sen_len_2 = input_2[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)입력 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input_1 ##\n",
    "\n",
    "input_vec_1 = layers.Input([sen_len_1])\n",
    "c_embedding = layers.Embedding(vocab_size, embedding_dim)(input_vec_1)\n",
    "\n",
    "position_encoding = layers.Lambda(lambda x : \n",
    "                                      get_timing_signal_1d(length = int(x.shape[1]), \n",
    "                                                           channels = int(x.shape[2])))(c_embedding)\n",
    "\n",
    "c_embedding = layers.Lambda(lambda x :\n",
    "                           x[0] + x[1])([c_embedding,position_encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input_2 ##\n",
    "\n",
    "input_vec_2 = layers.Input([sen_len_2])\n",
    "o_embedding = layers.Embedding(vocab_size, embedding_dim)(input_vec_2)\n",
    "\n",
    "position_encoding = layers.Lambda(lambda x : \n",
    "                                      get_timing_signal_1d(length = int(x.shape[1]), \n",
    "                                                           channels = int(x.shape[2])))(o_embedding)\n",
    "    \n",
    "o_embedding = layers.Lambda(lambda x :\n",
    "                           x[0] + x[1])([o_embedding,position_encoding])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Transformer 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1)encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, encoder_num):\n",
    "    print(i)\n",
    "\n",
    "    if i == 0:\n",
    "        queries = layers.Dense(dense_dim)(c_embedding)\n",
    "        keys = layers.Dense(dense_dim)(c_embedding)\n",
    "        values = layers.Dense(dense_dim)(c_embedding)\n",
    "        \n",
    "        query_key = layers.Dot(axes = 2)([queries,keys])\n",
    "        query_key = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key)\n",
    "        query_key = layers.Softmax()(query_key)\n",
    "        \n",
    "        query_key = layers.Lambda(lambda x : x + (create_padding_mask(input_vec_1) * (-1e9)))(query_key)\n",
    "        \n",
    "        attention = layers.Dot(axes=1)([query_key,values])\n",
    "        \n",
    "        for j in range(0 ,(multi_head_num - 1)):\n",
    "            queries_n = layers.Dense(dense_dim)(c_embedding)\n",
    "            keys_n = layers.Dense(dense_dim)(c_embedding)\n",
    "            values_n = layers.Dense(dense_dim)(c_embedding)\n",
    "\n",
    "            query_key_n = layers.Dot(axes = 2)([queries_n,keys_n])\n",
    "            query_key_n = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key_n)\n",
    "            query_key_n = layers.Softmax()(query_key_n)\n",
    "            \n",
    "            query_key_n = layers.Lambda(lambda x : x + (create_padding_mask(input_vec_1) * (-1e9)))(query_key_n)\n",
    "\n",
    "            attention_n = layers.Dot(axes=1)([query_key_n,values_n])\n",
    "\n",
    "            attention = layers.Concatenate()([attention,attention_n])\n",
    "            \n",
    "        attention_head = layers.Dense(embedding_dim)(attention)\n",
    "\n",
    "        encoder_resnet = layers.Add()([c_embedding,attention_head])\n",
    "        encoder_resnet = layers.BatchNormalization()(encoder_resnet)\n",
    "\n",
    "        encoder_output_resnet = layers.Dense(embedding_dim)(encoder_resnet)\n",
    "        encoder_output_resnet = layers.Add()([encoder_output_resnet, encoder_resnet])\n",
    "        encoder_output_resnet = layers.BatchNormalization()(encoder_output_resnet)\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        quries = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "        keys = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "        values = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "\n",
    "        query_key = layers.Dot(axes = 2)([queries,keys])\n",
    "        query_key = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key)\n",
    "        query_key = layers.Softmax()(query_key)\n",
    "        \n",
    "        query_key = layers.Lambda(lambda x : x + (create_padding_mask(input_vec_1) * (-1e9)))(query_key)\n",
    "\n",
    "        attention = layers.Dot(axes=1)([query_key,values])\n",
    "\n",
    "        for j in range(0 ,(multi_head_num - 1)):\n",
    "            queries_n = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "            keys_n = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "            values_n = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "\n",
    "            query_key_n = layers.Dot(axes = 2)([queries_n,keys_n])\n",
    "            query_key_n = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key_n)\n",
    "            query_key_n = layers.Softmax()(query_key_n)\n",
    "            \n",
    "            query_key_n = layers.Lambda(lambda x : x + (create_padding_mask(input_vec_1) * (-1e9)))(query_key_n)\n",
    "\n",
    "            attention_n = layers.Dot(axes=1)([query_key_n,values_n])\n",
    "\n",
    "            attention = layers.Concatenate()([attention,attention_n])\n",
    "\n",
    "        attention_head = layers.Dense(embedding_dim)(attention)\n",
    "\n",
    "        encoder_resnet = layers.Add()([encoder_output_resnet,attention_head])\n",
    "        encoder_resnet = layers.BatchNormalization()(encoder_resnet)\n",
    "\n",
    "        encoder_output_resnet = layers.Dense(embedding_dim)(encoder_resnet)\n",
    "        encoder_output_resnet = layers.Add()([encoder_output_resnet, encoder_resnet])\n",
    "        encoder_output_resnet = layers.BatchNormalization()(encoder_output_resnet)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    " for i in range(0, decoder_num):\n",
    "    \n",
    "    print(i)    \n",
    "    \n",
    "    ## first floor decoder\n",
    "    \n",
    "    if i == 0 :    \n",
    "        queries = layers.Dense(dense_dim)(o_embedding)\n",
    "        keys = layers.Dense(dense_dim)(o_embedding)\n",
    "        values = layers.Dense(dense_dim)(o_embedding)\n",
    "\n",
    "        query_key = layers.Dot(axes = 2)([keys,queries])\n",
    "        query_key = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key)\n",
    "        query_key = layers.Softmax()(query_key)\n",
    "        \n",
    "        combined_mask = tf.maximum((create_look_ahead_mask(input_vec_2.shape[1]) * (-1e9)), \n",
    "                   (create_look_ahead_mask(input_vec_2.shape[1]) * (-1e9)))\n",
    "        \n",
    "        query_key = layers.Lambda(lambda x : x + combined_mask)(query_key)\n",
    "        \n",
    "        attention = layers.Dot(axes=1)([query_key,values])\n",
    "        \n",
    "        for j in range(0 ,(multi_head_num - 1)):\n",
    "            \n",
    "            queries_n = layers.Dense(dense_dim)(o_embedding)\n",
    "            keys_n = layers.Dense(dense_dim)(o_embedding)\n",
    "            values_n = layers.Dense(dense_dim)(o_embedding)\n",
    "\n",
    "            query_key_n = layers.Dot(axes = 2)([keys_n,queries_n])\n",
    "            query_key_n = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key_n)\n",
    "            query_key_n = layers.Softmax()(query_key_n)\n",
    "            \n",
    "            query_key_n = layers.Lambda(lambda x : x + combined_mask)(query_key_n)\n",
    "\n",
    "            attention_n = layers.Dot(axes=1)([query_key_n,values_n])\n",
    "\n",
    "            attention = layers.Concatenate()([attention,attention_n])\n",
    "\n",
    "        attention_head = layers.Dense(embedding_dim)(attention)\n",
    "\n",
    "        decoder_resnet = layers.Add()([o_embedding,attention_head])\n",
    "        decoder_resnet = layers.BatchNormalization()(decoder_resnet)\n",
    "\n",
    "        decoder_output_resnet = layers.Dense(embedding_dim)(decoder_resnet)\n",
    "        decoder_output_resnet = layers.Add()([decoder_output_resnet, decoder_resnet])\n",
    "        decoder_output_resnet = layers.BatchNormalization()(decoder_output_resnet)\n",
    "        \n",
    "        queries = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "        keys = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "        values = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "        \n",
    "        query_key = layers.Dot(axes = 2)([keys,queries])\n",
    "        query_key = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key)\n",
    "        query_key = layers.Softmax()(query_key)\n",
    "        \n",
    "        query_key = layers.Lambda(lambda x : x + (create_padding_mask(input_vec_2) * (-1e9)))(query_key)\n",
    "\n",
    "        attention = layers.Dot(axes=1)([query_key,values])\n",
    "\n",
    "        for j in range(0 ,(multi_head_num - 1)):\n",
    "            queries_n = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "            keys_n = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "            values_n = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "\n",
    "            query_key_n = layers.Dot(axes = 2)([keys_n,queries_n])\n",
    "            query_key_n = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key_n)\n",
    "            query_key_n = layers.Softmax()(query_key_n)\n",
    "            \n",
    "            query_key_n = layers.Lambda(lambda x : x + (create_padding_mask(input_vec_2) * (-1e9)))(query_key_n)\n",
    "\n",
    "            attention_n = layers.Dot(axes=1)([query_key_n,values_n])\n",
    "\n",
    "            attention = layers.Concatenate()([attention,attention_n])\n",
    "\n",
    "        attention_head = layers.Dense(embedding_dim)(attention)\n",
    "\n",
    "        decoder_resnet = layers.Add()([decoder_output_resnet,attention_head])\n",
    "        decoder_resnet = layers.BatchNormalization()(decoder_resnet)\n",
    "\n",
    "        decoder_output_resnet = layers.Dense(embedding_dim)(decoder_resnet)\n",
    "        decoder_output_resnet = layers.Add()([decoder_output_resnet, decoder_resnet])\n",
    "        decoder_output_resnet = layers.BatchNormalization()(decoder_output_resnet)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "              ## n-th floor decoder  \n",
    "        \n",
    "        queries = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "        keys = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "        values = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "\n",
    "        query_key = layers.Dot(axes = 2)([keys,queries])\n",
    "        query_key = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key)\n",
    "        query_key = layers.Softmax()(query_key)\n",
    "        \n",
    "        combined_mask = tf.maximum((create_look_ahead_mask(input_vec_2.shape[1]) * (-1e9)), \n",
    "           (create_look_ahead_mask(input_vec_2.shape[1]) * (-1e9)))\n",
    "        \n",
    "        query_key = layers.Lambda(lambda x : x + combined_mask)(query_key)\n",
    "        \n",
    "        attention = layers.Dot(axes=1)([query_key,values])\n",
    "        \n",
    "        for j in range(0 ,(multi_head_num - 1)):\n",
    "            \n",
    "            queries_n = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "            keys_n = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "            values_n = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "\n",
    "            query_key_n = layers.Dot(axes = 2)([keys_n,queries_n])\n",
    "            query_key_n = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key_n)\n",
    "            query_key_n = layers.Softmax()(query_key_n)\n",
    "            \n",
    "            query_key_n = layers.Lambda(lambda x : x + combined_mask)(query_key_n)\n",
    "\n",
    "            attention_n = layers.Dot(axes=1)([query_key_n,values_n])\n",
    "\n",
    "            attention = layers.Concatenate()([attention,attention_n])\n",
    "\n",
    "        attention_head = layers.Dense(embedding_dim)(attention)\n",
    "\n",
    "        decoder_resnet = layers.Add()([decoder_output_resnet,attention_head])\n",
    "        decoder_resnet = layers.BatchNormalization()(decoder_resnet)\n",
    "\n",
    "        decoder_output_resnet = layers.Dense(embedding_dim)(decoder_resnet)\n",
    "        decoder_output_resnet = layers.Add()([decoder_output_resnet, decoder_resnet])\n",
    "        decoder_output_resnet = layers.BatchNormalization()(decoder_output_resnet)\n",
    "        \n",
    "        queries = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "        keys = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "        values = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "        \n",
    "        query_key = layers.Dot(axes = 2)([keys,queries])\n",
    "        query_key = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key)\n",
    "        query_key = layers.Softmax()(query_key)\n",
    "        \n",
    "        query_key = layers.Lambda(lambda x : x + (create_padding_mask(input_vec_2) * (-1e9)))(query_key)\n",
    "\n",
    "        attention = layers.Dot(axes=1)([query_key,values])\n",
    "\n",
    "        for j in range(0 ,(multi_head_num - 1)):\n",
    "            queries_n = layers.Dense(dense_dim)(decoder_output_resnet)\n",
    "            keys_n = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "            values_n = layers.Dense(dense_dim)(encoder_output_resnet)\n",
    "\n",
    "            query_key_n = layers.Dot(axes = 2)([keys_n,queries_n])\n",
    "            query_key_n = layers.Lambda(lambda x: (x / np.sqrt(dense_dim)))(query_key_n)\n",
    "            query_key_n = layers.Softmax()(query_key_n)\n",
    "            \n",
    "            query_key_n = layers.Lambda(lambda x : x + (create_padding_mask(input_vec_2) * (-1e9)))(query_key_n)\n",
    "\n",
    "            attention_n = layers.Dot(axes=1)([query_key_n,values_n])\n",
    "\n",
    "            attention = layers.Concatenate()([attention,attention_n])\n",
    "\n",
    "        attention_head = layers.Dense(embedding_dim)(attention)\n",
    "\n",
    "        decoder_resnet = layers.Add()([decoder_output_resnet,attention_head])\n",
    "        decoder_resnet = layers.BatchNormalization()(decoder_resnet)\n",
    "\n",
    "        decoder_output_resnet = layers.Dense(embedding_dim)(decoder_resnet)\n",
    "        decoder_output_resnet = layers.Add()([decoder_output_resnet, decoder_resnet])\n",
    "        decoder_output_resnet = layers.BatchNormalization()(decoder_output_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 출력 및 모델 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_classification\n",
    "\n",
    "output_vec = layers.GlobalAveragePooling1D()(decoder_output_resnet)\n",
    "\n",
    "output_vec = layers.Dense(output_vec.shape[1])(output_vec)\n",
    "\n",
    "output_vec = layers.Dense(target_size, activation=\"softmax\")(output_vec)\n",
    "\n",
    "transformer = Model(inputs = [input_vec_1,input_vec_2], outputs = [output_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generating\n",
    "\n",
    "output_vec = layers.Dense(target_size,activation=\"softmax\")(decoder_output_resnet)\n",
    "\n",
    "#output_vec = layers.Lambda(lambda x : x[:,0,:])(output_vec)\n",
    "\n",
    "transformer = Model(inputs = [input_vec_1,input_vec_2], outputs = [output_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer.save(\"tranformer_cls.h5\")\n",
    "transformer.save(\"tranformer.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transformer.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 867 samples, validate on 290 samples\n",
      "Epoch 1/50\n",
      "190/867 [=====>........................] - ETA: 2:00:18 - loss: 4.5250 - accuracy: 0.0632"
     ]
    }
   ],
   "source": [
    "#text_classification\n",
    "\n",
    "    transformer.fit([X_train, X_train] , \n",
    "                    y_train ,\n",
    "                    epochs=50,\n",
    "                    batch_size = 5, \n",
    "                    validation_data = ([X_test, X_test], y_test),\n",
    "                    callbacks = [early_stopping]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,940) :\n",
    "    y_train = np.array([np.array(\n",
    "    dummies_temp.merge(\n",
    "        pd.get_dummies(j), \n",
    "        how = \"outer\").fillna(0)\n",
    "                ) \n",
    "    for i,j in enumerate(X_train_2[i : i + 10])])\n",
    "    \n",
    "    j = np.random.randint(11,200) - 10\n",
    "    \n",
    "    y_test = np.array([np.array(\n",
    "    dummies_temp.merge(\n",
    "        pd.get_dummies(j), \n",
    "        how = \"outer\").fillna(0)\n",
    "                ) \n",
    "    for i,j in enumerate(X_val_2[j : j + 10])])\n",
    "    \n",
    "    #X_val_1_reshape = X_val_1[j].reshape((1,X_val_1[j].shape[0]))\n",
    "    #X_val_2_reshape = X_val_2[j].reshape((1,X_val_2[j].shape[0]))\n",
    "    \n",
    "\n",
    "    transformer.fit([X_train_1[i : i + 10], X_train_2[i : i + 10]] , \n",
    "                    y_train ,\n",
    "                    epochs=50,\n",
    "                    batch_size = 5, \n",
    "                    validation_data = ([X_val_1[j : j + 10], X_val_2[j : j + 10]], y_test),\n",
    "                    #callbacks = [early_stopping]\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
