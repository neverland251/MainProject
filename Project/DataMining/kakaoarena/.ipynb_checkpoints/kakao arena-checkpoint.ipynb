{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Layer\n",
    "from keras.layers import Input,Dense,Flatten,Embedding,Permute,Dot,Reshape\n",
    "from keras.layers.convolutional import Conv1D,MaxPooling1D,MaxPooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM,GRU\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대분류  메타데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_gn_all = pd.read_json('.//데이터//genre_gn_all.json', typ = 'series',encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_gn_all = pd.DataFrame(genre_gn_all, columns = ['gnr_name']).reset_index().rename(columns = {'index' : 'gnr_code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 대분류\n",
    "\n",
    "genre_gn_all[genre_gn_all[\"gnr_code\"].str[-2:] == \"00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 곡  메타데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_meta = pd.read_json(\".//데이터//song_meta.json\",typ = \"frame\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_meta.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수의 의미\n",
    "\n",
    "#### song_gn_dtl_gnr_basket : 상세 장르 코드\n",
    "#### issue_date : 곡 발매 일자 (yyyymmdd)\n",
    "#### album_name : 앨범 명\n",
    "#### album_id : 앨범 아이디\n",
    "#### artist_id_basket : 아티스트 아이디 (복수일 경우 띄어쓰기로 구분)\n",
    "#### song_name : 곡 명\n",
    "#### song_gn_gnr_basket : 대분류 장르코드\n",
    "#### artist_name_basket : 아티스트 명 (복수일 경우 띄어쓰기로 구분)\n",
    "#### id : 곡 아이디"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_gnr_map = song_meta.loc[:, ['id', 'song_gn_gnr_basket']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_gnr_map_unnest = np.dstack(\n",
    "    (\n",
    "        np.repeat(song_gnr_map.id.values, list(map(len, song_gnr_map.song_gn_gnr_basket))), \n",
    "        np.concatenate(song_gnr_map.song_gn_gnr_basket.values)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_gnr_map = pd.DataFrame(data = song_gnr_map_unnest[0], columns = song_gnr_map.columns)\n",
    "song_gnr_map['id'] = song_gnr_map['id'].astype(str)\n",
    "song_gnr_map.rename(columns = {'id' : 'song_id', 'song_gn_gnr_basket' : 'gnr_code'}, inplace = True)\n",
    "\n",
    "del song_gnr_map_unnest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "song_gnr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대부분은  하나에  하나씩  매핑이지만, 두  개  이상인  경우도  없지는  않다..\n",
    "\n",
    "song_gnr_map[\"song_id\"].value_counts().value_counts()\n",
    "\n",
    "# 후에  곡을  일부  추리면서  두  개  이상  매핑  사례가  얼마나  남는지  확인해보자.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_json(\".//데이터//train.json\",typ=\"frame\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_map_unnest = np.dstack(\n",
    "    (\n",
    "        np.repeat(rawdata.id.values, list(map(len, rawdata.songs))), \n",
    "        np.concatenate(rawdata.songs.values)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_map_unnest = pd.DataFrame(rawdata_map_unnest[0],columns = [\"id\",\"songs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "plt.hist(rawdata_map_unnest[\"id\"].value_counts(),[i*2 for i in range(1,101)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = rawdata_map_unnest[\"id\"].value_counts().reset_index()\n",
    "\n",
    "temp.columns = [\"id\",\"songs_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawdata_id_likecnt = pd.merge(pd.DataFrame(rawdata[[\"id\",\"like_cnt\"]]), temp, on =[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = rawdata_map_unnest[\"id\"].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"곡  갯수  이상치  이상  like_cnt : \",\n",
    "    len(rawdata_id_likecnt.loc[rawdata_id_likecnt[\"songs_num\"] > desc[\"75%\"] + 1.5  * (desc[\"75%\"] - desc[\"25%\"]),\"like_cnt\"]))\n",
    "print(\"곡  갯수  정상범위  like_cnt : \",\n",
    "     len(rawdata_id_likecnt.loc[rawdata_id_likecnt[\"songs_num\"] <= desc[\"75%\"] + 1.5  * (desc[\"75%\"] - desc[\"25%\"]),\"like_cnt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"곡 갯수  이상치  이상  like_cnt의  평균  :\",\n",
    "    np.mean(rawdata_id_likecnt.loc[rawdata_id_likecnt[\"songs_num\"] > desc[\"75%\"] + 1.5  * (desc[\"75%\"] - desc[\"25%\"]),\"like_cnt\"]))\n",
    "\n",
    "print(\"곡 갯수  정상범위  like_cnt의  평균  :\",\n",
    "np.mean(rawdata_id_likecnt.loc[rawdata_id_likecnt[\"songs_num\"] <= desc[\"75%\"] + 1.5  * (desc[\"75%\"] - desc[\"25%\"]),\"like_cnt\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 르벤 등분산  검정  결과  등분산이  가정되지  않음(p < .05)\n",
    "\n",
    "scipy.stats.levene(rawdata_id_likecnt.loc[rawdata_id_likecnt[\"songs_num\"] > desc[\"75%\"] + 1.5  * (desc[\"75%\"] - desc[\"25%\"]),\"like_cnt\"],\n",
    "                rawdata_id_likecnt.loc[rawdata_id_likecnt[\"songs_num\"] <= desc[\"75%\"] + 1.5  * (desc[\"75%\"] - desc[\"25%\"]),\"like_cnt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이분산  t_test 결과 Like_cnt는 수록된  곡  수가 많은  경우  평균이  유의미하게  다르다는  것을  나타낸다..\n",
    "\n",
    "scipy.stats.ttest_ind(\n",
    "    rawdata_id_likecnt.loc[rawdata_id_likecnt[\"songs_num\"] > desc[\"75%\"] + 1.5  * (desc[\"75%\"] - desc[\"25%\"]),\"like_cnt\"],\n",
    "    rawdata_id_likecnt.loc[rawdata_id_likecnt[\"songs_num\"] <= desc[\"75%\"] + 1.5  * (desc[\"75%\"] - desc[\"25%\"]),\"like_cnt\"],\n",
    "equal_var = False)\n",
    "\n",
    "del rawdata_map_unnest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## like_cnt를  하이퍼  파라미터로  활용할  수  있지  않을까??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_id_likecnt[\"like_cnt\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.boxplot(rawdata_id_likecnt[\"like_cnt\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(rawdata_id_likecnt_norm[\"like_cnt\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(rawdata_id_likecnt[\"like_cnt\"],[i * 20 for i in range(0,2500)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = rawdata_id_likecnt[\"like_cnt\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_id_likecnt_norm = rawdata_id_likecnt.loc[rawdata_id_likecnt[\"like_cnt\"] <= \n",
    "                                                 desc[\"75%\"] + 1.5 * (desc[\"75%\"] - desc[\"25%\"]),\n",
    "                                                 [\"like_cnt\",\"songs_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### like_cnt와  songs_num은 상관관계가  낮다..\n",
    "\n",
    "pd.DataFrame({\"like_cnt\" : ((rawdata_id_likecnt_norm[\"like_cnt\"] - rawdata_id_likecnt_norm[\"like_cnt\"].min()) / \n",
    "(rawdata_id_likecnt_norm[\"like_cnt\"].max() - rawdata_id_likecnt_norm[\"like_cnt\"].min())),\n",
    "              \"songs_num\" : ((rawdata_id_likecnt_norm[\"songs_num\"] - rawdata_id_likecnt_norm[\"songs_num\"].min()) / \n",
    "(rawdata_id_likecnt_norm[\"songs_num\"].max() - rawdata_id_likecnt_norm[\"songs_num\"].min()))}).corr()\n",
    "\n",
    "del rawdata_id_likecnt_norm\n",
    "del rawdata_id_likecnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_tag_unnest = np.dstack(\n",
    "    (\n",
    "        np.repeat(rawdata.id.values, list(map(len, rawdata.tags))), \n",
    "        np.concatenate(rawdata.tags.values)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_tag = pd.DataFrame(rawdata_tag_unnest[0],columns=[\"id\",\"tags\"])\n",
    "\n",
    "del rawdata_tag_unnest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = rawdata_tag.groupby(\"id\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tag는  대부분 10개  미만이고, 11개도  5개  경우밖엔  안된다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(rawdata_tag[\"tags\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tag의  총  갯수는  29160개  이다.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 태그  원-핫-인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 플레이리스트 아이디(id)와 수록곡(songs) 추출\n",
    "plylst_song_map = rawdata[['id', 'songs']]\n",
    "\n",
    "# unnest songs\n",
    "plylst_song_map_unnest = np.dstack(\n",
    "    (\n",
    "        np.repeat(plylst_song_map.id.values, list(map(len, plylst_song_map.songs))), \n",
    "        np.concatenate(plylst_song_map.songs.values)\n",
    "    )\n",
    ")\n",
    "\n",
    "# unnested 데이터프레임 생성 : plylst_song_map\n",
    "plylst_song_map = pd.DataFrame(data = plylst_song_map_unnest[0], columns = plylst_song_map.columns)\n",
    "plylst_song_map['id'] = plylst_song_map['id'].astype(str)\n",
    "plylst_song_map['songs'] = plylst_song_map['songs']\n",
    "\n",
    "# unnest 객체 제거\n",
    "del plylst_song_map_unnest\n",
    "\n",
    "\n",
    "# 1. 곡 별 수록된 플레이리스트 개수 count 테이블 생성 : song_plylst_cnt\n",
    "song_plylst_cnt = plylst_song_map.groupby('songs').id.nunique().reset_index(name = 'mapping_plylst_cnt')\n",
    "\n",
    "del plylst_song_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_del_list = set(song_plylst_cnt.loc[song_plylst_cnt[\"mapping_plylst_cnt\"] == 1,\"songs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_songs_deduction = Series([])\n",
    "rawdata_songs_deduction = rawdata[\"songs\"].apply(lambda x : list(set(x).difference(song_del_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#songs 제거 후 \n",
    "\n",
    "# 플레이리스트 아이디(id)와 수록곡(songs) 추출\n",
    "plylst_song_map = pd.concat([rawdata[\"id\"],rawdata_songs_deduction],axis=1)\n",
    "\n",
    "# unnest songs\n",
    "plylst_song_map_unnest = np.dstack(\n",
    "    (\n",
    "        np.repeat(plylst_song_map.id.values, list(map(len, plylst_song_map.songs))), \n",
    "        np.concatenate(plylst_song_map.songs.values)\n",
    "    )\n",
    ")\n",
    "\n",
    "# unnested 데이터프레임 생성 : plylst_song_map\n",
    "plylst_song_map = pd.DataFrame(data = plylst_song_map_unnest[0], columns = plylst_song_map.columns)\n",
    "plylst_song_map['id'] = plylst_song_map['id'].astype(str)\n",
    "plylst_song_map['songs'] = plylst_song_map['songs']\n",
    "\n",
    "# unnest 객체 제거\n",
    "del plylst_song_map_unnest\n",
    "\n",
    "# 1. 곡 별 수록된 플레이리스트 개수 count 테이블 생성 : song_plylst_cnt\n",
    "song_plylst_cnt_after = plylst_song_map.groupby('songs').id.nunique().reset_index(name = 'mapping_plylst_cnt')\n",
    "\n",
    "del plylst_song_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(rawdata[\"tags\"].apply(lambda x : math.floor(len(x) * 0.632)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spliter(x):\n",
    "    num = len(x)\n",
    "    r = math.floor(num * 0.632)\n",
    "    break_num = round(np.log((math.factorial(num)) / (math.factorial(r) * math.factorial(num - r))))\n",
    "    temp_1 = np.sort(np.random.choice(x,r,replace=False))\n",
    "    temp_2 = pd.DataFrame([np.sort(np.setdiff1d(x,temp_1).tolist())])\n",
    "    temp_1 = pd.DataFrame([temp_1.tolist()])\n",
    "    while True:\n",
    "        if r == 0:\n",
    "            temp_2_1 = pd.DataFrame([np.sort(np.setdiff1d(x,temp_1).tolist())])\n",
    "            temp_1 = pd.DataFrame([[]])\n",
    "            temp_2 = pd.concat([temp_2,temp_2_1]).drop_duplicates()\n",
    "            \n",
    "        if r != 0:\n",
    "            temp_1_1 = np.sort(np.random.choice(x,r,replace=False))\n",
    "            temp_2_1 = pd.DataFrame([np.sort(np.setdiff1d(x,temp_1_1).tolist())])\n",
    "            temp_1_1 = pd.DataFrame([temp_1_1.tolist()])\n",
    "            temp_1 = pd.concat([temp_1,temp_1_1],axis=0).drop_duplicates()\n",
    "            temp_2 = pd.concat([temp_2,temp_2_1]).drop_duplicates()\n",
    "        \n",
    "        if temp_2.shape[0] >= break_num:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return temp_1.values.tolist(),temp_2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spliter(x):\n",
    "    num = len(x)\n",
    "    r = math.floor(num * 0.632)\n",
    "    break_num = round(np.log((math.factorial(num)) / (math.factorial(r) * math.factorial(num - r))))\n",
    "    temp_1 = pd.DataFrame(np.sort(np.random.choice(x,r,replace=False)))\n",
    "    temp_2 = pd.DataFrame(x)\n",
    "    while True:\n",
    "        if r == 0:\n",
    "            temp_1 = pd.DataFrame(x)\n",
    "            \n",
    "        if r != 0:\n",
    "            temp_1_1 = pd.DataFrame(np.sort(np.random.choice(x,r,replace=False)))\n",
    "            temp_2_1 = pd.DataFrame(x)\n",
    "            temp_1 = pd.concat([temp_1,temp_1_1],axis=1)\n",
    "            temp_2 = pd.concat([temp_2,temp_2_1],axis=1)\n",
    "        \n",
    "        if temp_2.shape[0] >= break_num:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return temp_1.T.values.tolist(),temp_2.T.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_tag_split = pd.DataFrame([])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "temp = rawdata.copy()\n",
    "temp[\"songs\"] = rawdata_songs_deduction\n",
    "\n",
    "for i in range(0,len(temp)):\n",
    "\n",
    "    k1,k2 = spliter(temp.loc[i,\"tags\"])\n",
    "    temp_2 = pd.DataFrame([temp.iloc[i,:] for j in range(0,len(k2))])\n",
    "    temp_2[\"tags_input\"] = k1\n",
    "    temp_2[\"tags_output\"] = k2\n",
    "    rawdata_tag_split = pd.concat([rawdata_tag_split,temp_2],axis=0)\n",
    "    \n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(time.time() - start_time)\n",
    "        print((i / len(rawdata)) * 100, \"percent complete\")\n",
    "        start_time = time.time()\n",
    "\n",
    "del temp, temp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#songs 제거 후 \n",
    "\n",
    "# 플레이리스트 아이디(id)와 수록곡(songs) 추출\n",
    "plylst_song_map = pd.concat([rawdata_tag_split[\"id\"],rawdata_tag_split[\"songs\"]],axis=1)\n",
    "\n",
    "# unnest songs\n",
    "plylst_song_map_unnest = np.dstack(\n",
    "    (\n",
    "        np.repeat(plylst_song_map.id.values, list(map(len, plylst_song_map.songs))), \n",
    "        np.concatenate(plylst_song_map.songs.values)\n",
    "    )\n",
    ")\n",
    "\n",
    "# unnested 데이터프레임 생성 : plylst_song_map\n",
    "plylst_song_map = pd.DataFrame(data = plylst_song_map_unnest[0], columns = plylst_song_map.columns)\n",
    "plylst_song_map['id'] = plylst_song_map['id'].astype(str)\n",
    "plylst_song_map['songs'] = plylst_song_map['songs']\n",
    "\n",
    "# unnest 객체 제거\n",
    "del plylst_song_map_unnest\n",
    "\n",
    "# 1. 곡 별 수록된 플레이리스트 개수 count 테이블 생성 : song_plylst_cnt\n",
    "song_plylst_cnt_after = plylst_song_map.groupby('songs').id.nunique().reset_index(name = 'mapping_plylst_cnt')\n",
    "\n",
    "del plylst_song_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rawdata_tag_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-118f50688a7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrawdata_tag_split\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rawdata_tag_split_deduction_not_unique_tag.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rawdata_tag_split' is not defined"
     ]
    }
   ],
   "source": [
    "rawdata_tag_split.to_csv(\"rawdata_tag_split_deduction_not_unique_tag.csv\",index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_tag_split = pd.read_csv(\"rawdata_tag_split_deduction_not_unique_tag.csv\",encoding=\"utf-8\")\n",
    "\n",
    "rawdata_tag_split[\"tags_input\"] = rawdata_tag_split[\"tags_input\"].apply(lambda x : ast.literal_eval(x))\n",
    "rawdata_tag_split[\"tags_output\"] = rawdata_tag_split[\"tags_output\"].apply(lambda x : ast.literal_eval(x))\n",
    "rawdata_tag_split[\"tags\"] = rawdata_tag_split[\"tags\"].apply(lambda x : ast.literal_eval(x))\n",
    "rawdata_tag_split[\"songs\"] = rawdata_tag_split[\"songs\"].apply(lambda x : ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_input = rawdata_tag_split[\"tags_input\"].copy()\n",
    "song_input = rawdata_tag_split[\"songs\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse를 위해 mlb 절대 삭제하지 말것!\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb = mlb.fit(rawdata_tag_split[\"tags_output\"])\n",
    "mlb.sparse_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_output = mlb.transform(rawdata_tag_split[\"tags_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL-INPUt 전용\n",
    "\n",
    "tags_output = rawdata_tag_split[\"tags_output\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_to_inf(seq,length):\n",
    "    n = 0\n",
    "    while True:\n",
    "        temp = seq[n]\n",
    "        n = n+1\n",
    "        yield(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_title = rawdata_tag_split[\"plylst_title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "rawdata[\"plylst_title\"].to_csv(\"plylst_title.txt\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates= '--input={} \\\n",
    "--pad_id={} \\\n",
    "--bos_id={} \\\n",
    "--eos_id={} \\\n",
    "--unk_id={} \\\n",
    "--model_prefix={} \\\n",
    "--vocab_size={} \\\n",
    "--character_coverage={} \\\n",
    "--model_type={}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_file = \"./plylst_title.txt\"\n",
    "pad_id=0  #<pad> token을 0으로 설정\n",
    "vocab_size = 20000 # vocab 사이즈\n",
    "prefix = 'plylst_title' # 저장될 tokenizer 모델에 붙는 이름\n",
    "bos_id=1 #<start> token을 1으로 설정\n",
    "eos_id=2 #<end> token을 2으로 설정\n",
    "unk_id=3 #<unknown> token을 3으로 설정\n",
    "character_coverage = 1.0 # to reduce character set \n",
    "model_type ='unigram' # Choose from unigram (default), bpe, char, or word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = templates.format(train_input_file,\n",
    "                pad_id,\n",
    "                bos_id,\n",
    "                eos_id,\n",
    "                unk_id,\n",
    "                prefix,\n",
    "                vocab_size,\n",
    "                character_coverage,\n",
    "                model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_encoder = spm.SentencePieceProcessor()\n",
    "title_encoder.Load(\"plylst_title.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title_encoder.SetEncodeExtraOptions('bos:eos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_title_input = play_title.apply(lambda x : title_encoder.EncodeAsIds(x))\n",
    "play_title_input = DataFrame(play_title_input.tolist()).fillna(0).to_numpy(dtype=\"int\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_t_i = zero_to_inf([i for i in range(0,len(song_input))],len(song_input))\n",
    "k = tags_input.apply(lambda x : x  + song_input[next(z_t_i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_t_i = zero_to_inf([i for i in range(0,len(song_input))],len(song_input))\n",
    "k = k.apply(lambda x : x  + play_title_input[next(z_t_i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['락', '추억', '회상', ..., '팝송추천', '팝송', '팝송모음'], dtype='<U18')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(rawdata.tags.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_tag_unnest = np.dstack(\n",
    "    (\n",
    "        np.repeat(rawdata_tag_split.id.values, list(map(len, k))), \n",
    "        np.concatenate(k)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_tag = pd.DataFrame(rawdata_tag_unnest[0],columns=[\"id\",\"tags\"])\n",
    "\n",
    "del rawdata_tag_unnest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.DataFrame(pd.unique(rawdata_tag[\"tags\"]))\n",
    "\n",
    "dictionary_tags = pd.concat([Series([0]),dictionary]).reset_index(drop=True)\n",
    "\n",
    "del rawdata_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w : i for i, w in enumerate(dictionary_tags[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_input = list(map(lambda x : list(map(lambda y : word2idx[y],x)),tags_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_input = DataFrame(tags_input).fillna(0).to_numpy(dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_songs = pd.DataFrame(pd.unique(song_plylst_cnt_after[\"songs\"]))\n",
    "\n",
    "#dictionary_songs = pd.concat([Series([0]),dictionary]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w : i for i, w in enumerate(dictionary_songs[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_input = list(map(lambda x : list(map(lambda y : word2idx[y],x)),rawdata_tag_split[\"songs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_input = DataFrame(song_input).fillna(0).to_numpy(dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모든 인풋을 하나로 묶어서 처리(all_input)\n",
    "#### Input용\n",
    "\n",
    "\n",
    "\n",
    "play_title_input = play_title.apply(lambda x : title_encoder.EncodeAsPieces(x))\n",
    "#play_title_input = DataFrame(play_title_input.tolist()).fillna(0).to_numpy(dtype=\"int\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_t_i = zero_to_inf([i for i in range(0,len(song_input))],len(song_input))\n",
    "k = play_title_input.apply(lambda x : x  + song_input[next(z_t_i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_t_i = zero_to_inf([i for i in range(0,len(tags_input))],len(tags_input))\n",
    "inputs = k.apply(lambda x : x  + tags_input[next(z_t_i)])\n",
    "\n",
    "z_t_i = zero_to_inf([i for i in range(0,len(tags_output))],len(tags_output))\n",
    "outputs = k.apply(lambda x : x  + tags_output[next(z_t_i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_t_i = zero_to_inf([i for i in range(0,len(tags_output))],len(tags_output))\n",
    "\n",
    "all_for_dict = inputs.apply(lambda x : x  + outputs[next(z_t_i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_tag_unnest = np.dstack(\n",
    "    (\n",
    "        np.repeat(rawdata_tag_split.id.values, list(map(len, all_for_dict))), \n",
    "        np.concatenate(all_for_dict)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_tag = pd.DataFrame(rawdata_tag_unnest[0],columns=[\"id\",\"tags\"])\n",
    "\n",
    "del rawdata_tag_unnest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.DataFrame(pd.unique(rawdata_tag[\"tags\"]))\n",
    "\n",
    "dictionary_all = pd.concat([Series([\"0\"]),dictionary]).reset_index(drop=True)\n",
    "\n",
    "del rawdata_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {w : i for i, w in enumerate(dictionary_all[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input = list(map(lambda x : list(map(lambda y : word2idx[str(y)],x)),inputs))\n",
    "all_output = list(map(lambda x : list(map(lambda y : word2idx[str(y)],x)),outputs))\n",
    "all_for_dict = list(map(lambda x : list(map(lambda y : word2idx[str(y)],x)),all_for_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_unique(x):\n",
    "    total = x.shape[0]\n",
    "    num = 0\n",
    "    temp = np.where(x.toarray()[0] == 1)[0]\n",
    "    for i in range(0,len(temp)):\n",
    "        num = num + np.log(temp[i] + 0.00001)\n",
    "    \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = [label_unique(i) for i in tag_one_hot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata[\"unique_list\"] = unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawdata[\"unique_list\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train - test 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_for_split = DataFrame([Series(play_title_input.tolist(),name=\"plylst_title\"),\n",
    "           Series(tags_input.tolist(),name=\"tags\"),\n",
    "           Series(song_input.tolist(),name=\"songs\")]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(concat_for_split, tags_output,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_2_train = MultiLabelBinarizer()\n",
    "mlb_2_train = mlb_2_train.fit(X_train[\"tags\"])\n",
    "mlb_2_train.sparse_output = True\n",
    "\n",
    "mlb_2_test = MultiLabelBinarizer()\n",
    "mlb_2_test = mlb_2_test.fit(X_test[\"tags\"])\n",
    "mlb_2_test.sparse_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tags = mlb_2_train.transform(X_train[\"tags\"])\n",
    "X_test_tags = mlb_2_test.transform(X_test[\"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title = np.array(X_train[\"plylst_title\"].tolist())\n",
    "X_train_songs = np.array(X_train[\"songs\"].tolist())\n",
    "X_train_tags = np.array(X_train[\"tags\"].tolist())\n",
    "\n",
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_title = np.array(X_test[\"plylst_title\"].tolist())\n",
    "X_test_songs = np.array(X_test[\"songs\"].tolist())\n",
    "X_test_tags = np.array(X_test[\"tags\"].tolist())\n",
    "\n",
    "del X_test, concat_for_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./plylst_title.vocab', encoding='utf-8') as f:\n",
    "    Vo = [doc.strip().split(\"\\t\") for doc in f]\n",
    "\n",
    "# w[0]: token name    \n",
    "# w[1]: token score\n",
    "word2idx = {w[0]: i for i, w in enumerate(Vo)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All_input 전용\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(all_input, all_output ,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rawdata_tag_split, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_2 = MultiLabelBinarizer()\n",
    "mlb_2 = mlb_2.fit(all_for_dict)\n",
    "mlb_2.sparse_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mlb_2.transform(X_train)\n",
    "\n",
    "X_test = mlb_2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = mlb_2.transform(y_train)\n",
    "\n",
    "y_test = mlb_2.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델  구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1_shape = X_train.shape[1]\n",
    "\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "\n",
    "inputs_1 = layers.Input(shape=[inputs_1_shape],name = \"Feed_Sentence\")\n",
    "\n",
    "encoder_dense_layer = layers.Dense(100)\n",
    "\n",
    "encoder_dense_1 = encoder_dense_layer(inputs_1)\n",
    "\n",
    "encoder_dense_1 = layers.BatchNormalization(axis = 1)(encoder_dense_1)\n",
    "\n",
    "#reshape_for_add = layers.Dense(100,kernel_initializer = \"ones\",name = \"reshape_for_add_1\",trainable=False)(inputs_1)\n",
    "\n",
    "#add_dense = layers.Add(name = \"add_for_ResNet\")([encoder_dense_1,reshape_for_add])\n",
    "\n",
    "encoder_dense_1 = layers.Activation(activation=\"relu\")(encoder_dense_1)\n",
    "\n",
    "encoder_dense_layer_2 = layers.Dense(50)\n",
    "\n",
    "encoder_dense_2 = encoder_dense_layer_2(encoder_dense_1)\n",
    "\n",
    "encoder_dense_2 = layers.BatchNormalization(axis = 1)(encoder_dense_2)\n",
    "\n",
    "#reshape_for_add_2 = layers.Dense(50,kernel_initializer = \"ones\",name = \"reshape_for_add_2\",trainable=False)(encoder_dense_1)\n",
    "\n",
    "#add_dense_2 = layers.Add(name = \"add_for_ResNet_2\")([encoder_dense_2,reshape_for_add_2])\n",
    "\n",
    "encoder_dense_2 = layers.Activation(activation=\"relu\")(encoder_dense_2)\n",
    "\n",
    "embed_dense_layer = layers.Dense(30)\n",
    "\n",
    "embed_dense = embed_dense_layer(encoder_dense_2)\n",
    "\n",
    "embed_dense= layers.BatchNormalization(axis = 1)(embed_dense)\n",
    "\n",
    "embed_dense = layers.Activation(activation=\"relu\")(embed_dense)\n",
    "\n",
    "decoder_dense = layers.DenseTied(50,tied_to=embed_dense_layer.kernel,use_bias=False)(embed_dense)\n",
    "\n",
    "decoder_dense = layers.BatchNormalization(axis = 1)(decoder_dense)\n",
    "\n",
    "#reshape_for_add_3 = layers.Dense(50,kernel_initializer = \"ones\",name = \"reshape_for_add_3\",trainable=False)(embed_dense)\n",
    "\n",
    "#decoder_dense = layers.Add(name = \"add_for_ResNet_3\")([decoder_dense,reshape_for_add_3])\n",
    "\n",
    "decoder_dense_2 = layers.DenseTied(100,tied_to = encoder_dense_layer_2.kernel,use_bias=False)(decoder_dense)\n",
    "\n",
    "decoder_dense_2 = layers.BatchNormalization(axis = 1)(decoder_dense_2)\n",
    "\n",
    "#reshape_for_add_4 = layers.Dense(100,kernel_initializer = \"ones\",name = \"reshape_for_add_4\",trainable=False)(decoder_dense)\n",
    "\n",
    "#decoder_dense_2 = layers.Add(name = \"add_for_ResNet_4\")([decoder_dense_2,reshape_for_add_4])\n",
    "\n",
    "output = layers.DenseTied(output_shape,tied_to = encoder_dense_layer.kernel,use_bias=False)(decoder_dense_2)\n",
    "\n",
    "output = layers.BatchNormalization(axis = 1)(output)\n",
    "\n",
    "#reshape_for_add_5 = layers.Dense(output_shape,kernel_initializer = \"ones\",name = \"reshape_for_add_5\",trainable=False)(decoder_dense_2)\n",
    "\n",
    "#output = layers.Add(name = \"add_for_ResNet_5\")([output ,reshape_for_add_5])\n",
    "\n",
    "#output = layers.Activation(activation = \"relu\")(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Feed_Sentence (InputLayer)   (None, 362328)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               36232900  \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 30)                1530      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_tied_7 (DenseTied)     (None, 50)                3000      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_tied_8 (DenseTied)     (None, 100)               10000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_tied_9 (DenseTied)     (None, 362328)            72465600  \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 362328)            1449312   \n",
      "=================================================================\n",
      "Total params: 110,168,712\n",
      "Trainable params: 73,204,096\n",
      "Non-trainable params: 36,964,616\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tag_identifier = Model(inputs = [inputs_1], outputs = [output])\n",
    "\n",
    "tag_identifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정답 벡터가 26000차원의 Sparse matrix이므로, 손실함수는 sparse_categorical_crossentropy를 사용한다.\n",
    "\n",
    "tag_identifier.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 142030 samples, validate on 60870 samples\n",
      "Epoch 1/20\n",
      "   768/142030 [..............................] - ETA: 1:09:59 - loss: 748.9539 - acc: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-1d473b4c4cf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m            (X_test,\n\u001b[0;32m      7\u001b[0m             y_test),\n\u001b[1;32m----> 8\u001b[1;33m            callbacks = [early_stopping])\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tag_identifier.fit(X_train\n",
    "           ,y_train,\n",
    "           epochs=20,\n",
    "           batch_size = 64,\n",
    "           validation_data = \n",
    "           (X_test,\n",
    "            y_test),\n",
    "           callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00289193,  0.00121131, -0.00032793, ..., -0.00175194,\n",
       "         0.00303095,  0.00026498],\n",
       "       [-0.00231144,  0.00368544,  0.00141668, ..., -0.0016287 ,\n",
       "        -0.00139199,  0.00050356],\n",
       "       [ 0.00277645, -0.00306646,  0.00292625, ...,  0.00281179,\n",
       "        -0.00205548,  0.00211462],\n",
       "       ...,\n",
       "       [ 0.00188192, -0.00096183, -0.00194065, ..., -0.00208827,\n",
       "         0.00083379, -0.00183137],\n",
       "       [ 0.00381338, -0.00318751, -0.00349205, ..., -0.0024775 ,\n",
       "         0.00360971, -0.00262959],\n",
       "       [-0.00334783,  0.00399643,  0.0023639 , ..., -0.0017095 ,\n",
       "        -0.00025111, -0.00173873]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_identifier.layers[1].get_weights()[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00057171, -0.00297893,  0.00261756, ...,  0.00082402,\n",
       "         -0.0005985 ,  0.00359352],\n",
       "        [-0.00380454,  0.00032644,  0.00332809, ..., -0.00017911,\n",
       "          0.00147678,  0.00370153],\n",
       "        [ 0.00163933,  0.00216155, -0.0040303 , ...,  0.00046308,\n",
       "         -0.00028749,  0.00204777],\n",
       "        ...,\n",
       "        [-0.00230674,  0.00028895,  0.00222251, ...,  0.00342184,\n",
       "          0.00042069, -0.001936  ],\n",
       "        [ 0.003787  ,  0.00219885, -0.00257533, ...,  0.00221136,\n",
       "          0.00320632,  0.0029215 ],\n",
       "        [-0.00376333, -0.00232992, -0.00247567, ...,  0.0014046 ,\n",
       "          0.00286406, -0.00108643]], dtype=float32),\n",
       " array([[ 0.00289193,  0.00121131, -0.00032793, ..., -0.00175194,\n",
       "          0.00303095,  0.00026498],\n",
       "        [-0.00231144,  0.00368544,  0.00141668, ..., -0.0016287 ,\n",
       "         -0.00139199,  0.00050356],\n",
       "        [ 0.00277645, -0.00306646,  0.00292625, ...,  0.00281179,\n",
       "         -0.00205548,  0.00211462],\n",
       "        ...,\n",
       "        [ 0.00188192, -0.00096183, -0.00194065, ..., -0.00208827,\n",
       "          0.00083379, -0.00183137],\n",
       "        [ 0.00381338, -0.00318751, -0.00349205, ..., -0.0024775 ,\n",
       "          0.00360971, -0.00262959],\n",
       "        [-0.00334783,  0.00399643,  0.0023639 , ..., -0.0017095 ,\n",
       "         -0.00025111, -0.00173873]], dtype=float32)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_identifier.layers[14].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = layers.Input(shape=[260],name = \"Feed_Sentence\")\n",
    "\n",
    "embed = layers.Embedding(300000,128)\n",
    "\n",
    "embed_i = embed(inputs_1)\n",
    "                \n",
    "encoder_dense = layers.Flatten()(embed_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_title' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1caf00555c93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minputs_1_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_title\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0minputs_2_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_songs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minputs_3_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_tags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msong_embedding_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary_songs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_title' is not defined"
     ]
    }
   ],
   "source": [
    "inputs_1_shape = X_train_title.shape[1]\n",
    "inputs_2_shape = X_train_songs.shape[1]\n",
    "inputs_3_shape = X_train_tags.shape[1]\n",
    "\n",
    "song_embedding_shape = len(dictionary_songs)\n",
    "tag_embedding_shape = len(dictionary)\n",
    "\n",
    "output_shape = tags_output.shape[1]\n",
    "#tf.reset_default_graph\n",
    "\n",
    "## GRU 입력 전 사전처리 모듈(임베딩 -> 컨볼루션 -> 맥스풀링)\n",
    "\n",
    "inputs_1 = layers.Input(shape=[inputs_1_shape],name = \"Feed_Sentence\")\n",
    "\n",
    "embed = layers.Embedding(len(word2idx),128)\n",
    "\n",
    "embed_i = embed(inputs_1)\n",
    "\n",
    "#Bi-GRU 인코더 - 디코더 네트워크\n",
    "Encoder1 = layers.GRU(128,input_shape=(None,128),name=\"Encoder1\")\n",
    "Encoder2 = layers.GRU(128,go_backwards = True,input_shape=(None,128),name=\"Encoder2\")\n",
    "initial_1 = Encoder1(embed_i)\n",
    "initial_2 = Encoder2(embed_i)\n",
    "\n",
    "title_initial_concat = layers.Concatenate(axis=-1,name = \"attention_matrix\")([initial_1,initial_2])\n",
    "\n",
    "context_vector = layers.LayerNormalization(axis = 1)(title_initial_concat)\n",
    "\n",
    "\n",
    "inputs_2 = layers.Input(shape = [inputs_2_shape],name = \"songs_input\")\n",
    "\n",
    "#곡에 대한 임베딩  레이어를 설정한다.\n",
    "\n",
    "embed_2 = layers.Embedding(input_dim = song_embedding_shape + 1 , output_dim = 128)\n",
    "\n",
    "embed_song = embed_2(inputs_2)\n",
    "\n",
    "#임베딩 조회를 마치고 나온 임베딩 벡터들의 관계를 학습한다.\n",
    "\n",
    "\n",
    "Encoder3 = layers.GRU(128,input_shape=(None,128),name=\"Encoder3\")\n",
    "Encoder4 = layers.GRU(128,go_backwards = True,input_shape=(None,128),name=\"Encoder4\")\n",
    "\n",
    "initial_3 = Encoder3(embed_song)\n",
    "initial_4 = Encoder4(embed_song)\n",
    "\n",
    "song_initial_concat = layers.Concatenate(axis = -1,name=\"song_initial_state_concat\")([initial_3,initial_4])\n",
    "\n",
    "song_dense = layers.LayerNormalization(axis = 1)(song_initial_concat)\n",
    "\n",
    "inputs_3 = layers.Input(shape = [inputs_3_shape],name = \"tags_input\")\n",
    "\n",
    "# 태그에 대한 임베딩 레이어를 설정한다.\n",
    "\n",
    "embed_3 = layers.Embedding(tag_embedding_shape+1,128)\n",
    "\n",
    "embed_tag = embed_3(inputs_3)\n",
    "\n",
    "Encoder5 = layers.GRU(128,input_shape=(None,128),name=\"Encoder5\")\n",
    "Encoder6 = layers.GRU(128,go_backwards = True,input_shape=(None,128),name=\"Encoder6\")\n",
    "\n",
    "initial_5 = Encoder5(embed_tag)\n",
    "initial_6 = Encoder6(embed_tag)\n",
    "\n",
    "tag_initial_concat = layers.Concatenate(axis = -1,name=\"tag_initial_state_concat\")([initial_5,initial_6])\n",
    "\n",
    "tag_dense = layers.LayerNormalization(axis = 1)(tag_initial_concat)\n",
    "\n",
    "\n",
    "\n",
    "# 두 벡터를  concat 하는  방안과,, add 하는  방안  두개를  모두  검토하자..\n",
    "## concat : 두  Input Vector의  가중치가  분리된다..\n",
    "## add : 두  Input Vector가  동일한  가중치를  공유한다..\n",
    "\n",
    "concat_vector = layers.Concatenate(axis = -1, name = \"concate_three_vector\")([context_vector,\n",
    "                                                                              song_dense,\n",
    "                                                                                tag_dense\n",
    "                                                                             ])\n",
    "\n",
    "predicts = layers.Dense(output_shape,activation=\"relu\")(concat_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Feed_Sentence (InputLayer)      (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "songs_input (InputLayer)        (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tags_input (InputLayer)         (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 42, 128)      2560000     Feed_Sentence[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 200, 128)     40462720    songs_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 6, 128)       3732608     tags_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Encoder1 (GRU)                  (None, 128)          98688       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Encoder2 (GRU)                  (None, 128)          98688       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Encoder3 (GRU)                  (None, 128)          98688       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Encoder4 (GRU)                  (None, 128)          98688       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Encoder5 (GRU)                  (None, 128)          98688       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Encoder6 (GRU)                  (None, 128)          98688       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_matrix (Concatenate)  (None, 256)          0           Encoder1[0][0]                   \n",
      "                                                                 Encoder2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "song_initial_state_concat (Conc (None, 256)          0           Encoder3[0][0]                   \n",
      "                                                                 Encoder4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tag_initial_state_concat (Conca (None, 256)          0           Encoder5[0][0]                   \n",
      "                                                                 Encoder6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256)          1024        attention_matrix[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        song_initial_state_concat[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        tag_initial_state_concat[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concate_three_vector (Concatena (None, 768)          0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 29160)        22424040    concate_three_vector[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 69,774,568\n",
      "Trainable params: 69,773,032\n",
      "Non-trainable params: 1,536\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#다중 라벨 분류를 위해선 각 정답 노드들의 활성화 함수를 sigmoid로 잡는다. \n",
    "\n",
    "\n",
    "tag_identifier_1 = Model(inputs = [inputs_1,\n",
    "                         inputs_2,\n",
    "                         inputs_3,\n",
    "                        ], \n",
    "               outputs = [predicts])\n",
    "tag_identifier_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정답 벡터가 26000차원의 Sparse matrix이므로, 손실함수는 sparse_categorical_crossentropy를 사용한다.\n",
    "\n",
    "tag_identifier_1.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"553pt\" viewBox=\"0.00 0.00 1722.00 553.00\" width=\"1722pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 549)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-549 1718,-549 1718,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2308638748288 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2308638748288</title>\n",
       "<polygon fill=\"none\" points=\"125,-498.5 125,-544.5 428,-544.5 428,-498.5 125,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"210\" y=\"-517.8\">Feed_Sentence: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"295,-498.5 295,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"295,-521.5 351,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"323\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"351,-498.5 351,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"389.5\" y=\"-529.3\">(None, 42)</text>\n",
       "<polyline fill=\"none\" points=\"351,-521.5 428,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"389.5\" y=\"-506.3\">(None, 42)</text>\n",
       "</g>\n",
       "<!-- 2308638748344 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2308638748344</title>\n",
       "<polygon fill=\"none\" points=\"115,-415.5 115,-461.5 438,-461.5 438,-415.5 115,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"196.5\" y=\"-434.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"278,-415.5 278,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"278,-438.5 334,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"306\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"334,-415.5 334,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"386\" y=\"-446.3\">(None, 42)</text>\n",
       "<polyline fill=\"none\" points=\"334,-438.5 438,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"386\" y=\"-423.3\">(None, 42, 128)</text>\n",
       "</g>\n",
       "<!-- 2308638748288&#45;&gt;2308638748344 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2308638748288-&gt;2308638748344</title>\n",
       "<path d=\"M276.5,-498.366C276.5,-490.152 276.5,-480.658 276.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"280,-471.607 276.5,-461.607 273,-471.607 280,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308638832288 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2308638832288</title>\n",
       "<polygon fill=\"none\" points=\"792,-498.5 792,-544.5 1083,-544.5 1083,-498.5 792,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"868\" y=\"-517.8\">songs_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"944,-498.5 944,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"972\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"944,-521.5 1000,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"972\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1000,-498.5 1000,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1041.5\" y=\"-529.3\">(None, 200)</text>\n",
       "<polyline fill=\"none\" points=\"1000,-521.5 1083,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1041.5\" y=\"-506.3\">(None, 200)</text>\n",
       "</g>\n",
       "<!-- 2308676583040 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2308676583040</title>\n",
       "<polygon fill=\"none\" points=\"772.5,-415.5 772.5,-461.5 1102.5,-461.5 1102.5,-415.5 772.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"854\" y=\"-434.8\">embedding_2: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"935.5,-415.5 935.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"963.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"935.5,-438.5 991.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"963.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"991.5,-415.5 991.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1047\" y=\"-446.3\">(None, 200)</text>\n",
       "<polyline fill=\"none\" points=\"991.5,-438.5 1102.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1047\" y=\"-423.3\">(None, 200, 128)</text>\n",
       "</g>\n",
       "<!-- 2308638832288&#45;&gt;2308676583040 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2308638832288-&gt;2308676583040</title>\n",
       "<path d=\"M937.5,-498.366C937.5,-490.152 937.5,-480.658 937.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"941,-471.607 937.5,-461.607 934,-471.607 941,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308678448128 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2308678448128</title>\n",
       "<polygon fill=\"none\" points=\"1244.5,-498.5 1244.5,-544.5 1512.5,-544.5 1512.5,-498.5 1244.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1315.5\" y=\"-517.8\">tags_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"1386.5,-498.5 1386.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1414.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1386.5,-521.5 1442.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1414.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1442.5,-498.5 1442.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1477.5\" y=\"-529.3\">(None, 6)</text>\n",
       "<polyline fill=\"none\" points=\"1442.5,-521.5 1512.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1477.5\" y=\"-506.3\">(None, 6)</text>\n",
       "</g>\n",
       "<!-- 2309530883240 -->\n",
       "<g class=\"node\" id=\"node6\"><title>2309530883240</title>\n",
       "<polygon fill=\"none\" points=\"1220,-415.5 1220,-461.5 1537,-461.5 1537,-415.5 1220,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1301.5\" y=\"-434.8\">embedding_3: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"1383,-415.5 1383,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1411\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1383,-438.5 1439,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1411\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1439,-415.5 1439,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1488\" y=\"-446.3\">(None, 6)</text>\n",
       "<polyline fill=\"none\" points=\"1439,-438.5 1537,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1488\" y=\"-423.3\">(None, 6, 128)</text>\n",
       "</g>\n",
       "<!-- 2308678448128&#45;&gt;2309530883240 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2308678448128-&gt;2309530883240</title>\n",
       "<path d=\"M1378.5,-498.366C1378.5,-490.152 1378.5,-480.658 1378.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1382,-471.607 1378.5,-461.607 1375,-471.607 1382,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308638748232 -->\n",
       "<g class=\"node\" id=\"node7\"><title>2308638748232</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 267,-378.5 267,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"53.5\" y=\"-351.8\">Encoder1: GRU</text>\n",
       "<polyline fill=\"none\" points=\"107,-332.5 107,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"107,-355.5 163,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"135\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"163,-332.5 163,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215\" y=\"-363.3\">(None, 42, 128)</text>\n",
       "<polyline fill=\"none\" points=\"163,-355.5 267,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215\" y=\"-340.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 2308638748344&#45;&gt;2308638748232 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2308638748344-&gt;2308638748232</title>\n",
       "<path d=\"M237.425,-415.366C220.085,-405.544 199.517,-393.894 181.273,-383.56\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"182.954,-380.49 172.528,-378.607 179.504,-386.581 182.954,-380.49\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308638831728 -->\n",
       "<g class=\"node\" id=\"node8\"><title>2308638831728</title>\n",
       "<polygon fill=\"none\" points=\"285,-332.5 285,-378.5 552,-378.5 552,-332.5 285,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"338.5\" y=\"-351.8\">Encoder2: GRU</text>\n",
       "<polyline fill=\"none\" points=\"392,-332.5 392,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"392,-355.5 448,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"448,-332.5 448,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500\" y=\"-363.3\">(None, 42, 128)</text>\n",
       "<polyline fill=\"none\" points=\"448,-355.5 552,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500\" y=\"-340.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 2308638748344&#45;&gt;2308638831728 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>2308638748344-&gt;2308638831728</title>\n",
       "<path d=\"M315.302,-415.366C332.364,-405.634 352.574,-394.106 370.566,-383.842\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"372.793,-386.602 379.745,-378.607 369.324,-380.522 372.793,-386.602\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308676475984 -->\n",
       "<g class=\"node\" id=\"node9\"><title>2308676475984</title>\n",
       "<polygon fill=\"none\" points=\"583.5,-332.5 583.5,-378.5 857.5,-378.5 857.5,-332.5 583.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"637\" y=\"-351.8\">Encoder3: GRU</text>\n",
       "<polyline fill=\"none\" points=\"690.5,-332.5 690.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"690.5,-355.5 746.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"718.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"746.5,-332.5 746.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"802\" y=\"-363.3\">(None, 200, 128)</text>\n",
       "<polyline fill=\"none\" points=\"746.5,-355.5 857.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"802\" y=\"-340.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 2308676583040&#45;&gt;2308676475984 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>2308676583040-&gt;2308676475984</title>\n",
       "<path d=\"M878.49,-415.473C850.865,-405.162 817.792,-392.816 789.061,-382.092\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"790.09,-378.74 779.498,-378.522 787.642,-385.298 790.09,-378.74\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308677145376 -->\n",
       "<g class=\"node\" id=\"node10\"><title>2308677145376</title>\n",
       "<polygon fill=\"none\" points=\"875.5,-332.5 875.5,-378.5 1149.5,-378.5 1149.5,-332.5 875.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"929\" y=\"-351.8\">Encoder4: GRU</text>\n",
       "<polyline fill=\"none\" points=\"982.5,-332.5 982.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1010.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"982.5,-355.5 1038.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1010.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1038.5,-332.5 1038.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094\" y=\"-363.3\">(None, 200, 128)</text>\n",
       "<polyline fill=\"none\" points=\"1038.5,-355.5 1149.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1094\" y=\"-340.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 2308676583040&#45;&gt;2308677145376 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>2308676583040-&gt;2308677145376</title>\n",
       "<path d=\"M957.994,-415.366C966.344,-406.348 976.122,-395.788 985.073,-386.121\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"987.805,-388.322 992.031,-378.607 982.668,-383.567 987.805,-388.322\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2309531317864 -->\n",
       "<g class=\"node\" id=\"node11\"><title>2309531317864</title>\n",
       "<polygon fill=\"none\" points=\"1174,-332.5 1174,-378.5 1435,-378.5 1435,-332.5 1174,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1227.5\" y=\"-351.8\">Encoder5: GRU</text>\n",
       "<polyline fill=\"none\" points=\"1281,-332.5 1281,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1309\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1281,-355.5 1337,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1309\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1337,-332.5 1337,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1386\" y=\"-363.3\">(None, 6, 128)</text>\n",
       "<polyline fill=\"none\" points=\"1337,-355.5 1435,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1386\" y=\"-340.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 2309530883240&#45;&gt;2309531317864 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>2309530883240-&gt;2309531317864</title>\n",
       "<path d=\"M1358.28,-415.366C1350.04,-406.348 1340.39,-395.788 1331.56,-386.121\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1334.03,-383.629 1324.7,-378.607 1328.86,-388.35 1334.03,-383.629\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2309530983616 -->\n",
       "<g class=\"node\" id=\"node12\"><title>2309530983616</title>\n",
       "<polygon fill=\"none\" points=\"1453,-332.5 1453,-378.5 1714,-378.5 1714,-332.5 1453,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1506.5\" y=\"-351.8\">Encoder6: GRU</text>\n",
       "<polyline fill=\"none\" points=\"1560,-332.5 1560,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1588\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1560,-355.5 1616,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1588\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1616,-332.5 1616,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1665\" y=\"-363.3\">(None, 6, 128)</text>\n",
       "<polyline fill=\"none\" points=\"1616,-355.5 1714,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1665\" y=\"-340.3\">(None, 128)</text>\n",
       "</g>\n",
       "<!-- 2309530883240&#45;&gt;2309530983616 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>2309530883240-&gt;2309530983616</title>\n",
       "<path d=\"M1434.25,-415.473C1460.23,-405.206 1491.32,-392.924 1518.38,-382.232\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1519.75,-385.452 1527.76,-378.522 1517.18,-378.942 1519.75,-385.452\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308586148136 -->\n",
       "<g class=\"node\" id=\"node13\"><title>2308586148136</title>\n",
       "<polygon fill=\"none\" points=\"209,-249.5 209,-295.5 614,-295.5 614,-249.5 209,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-268.8\">attention_matrix: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"391,-249.5 391,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"419\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"391,-272.5 447,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"419\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"447,-249.5 447,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"530.5\" y=\"-280.3\">[(None, 128), (None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"447,-272.5 614,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"530.5\" y=\"-257.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 2308638748232&#45;&gt;2308586148136 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>2308638748232-&gt;2308586148136</title>\n",
       "<path d=\"M209.098,-332.473C245.258,-321.937 288.705,-309.279 326.059,-298.395\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"327.296,-301.68 335.918,-295.522 325.338,-294.959 327.296,-301.68\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308638831728&#45;&gt;2308586148136 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>2308638831728-&gt;2308586148136</title>\n",
       "<path d=\"M416.587,-332.366C415.877,-324.152 415.057,-314.658 414.285,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"417.759,-305.268 413.41,-295.607 410.785,-305.871 417.759,-305.268\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308676275168 -->\n",
       "<g class=\"node\" id=\"node14\"><title>2308676275168</title>\n",
       "<polygon fill=\"none\" points=\"632.5,-249.5 632.5,-295.5 1088.5,-295.5 1088.5,-249.5 632.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"749\" y=\"-268.8\">song_initial_state_concat: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"865.5,-249.5 865.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"893.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"865.5,-272.5 921.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"893.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"921.5,-249.5 921.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1005\" y=\"-280.3\">[(None, 128), (None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"921.5,-272.5 1088.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1005\" y=\"-257.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 2308676475984&#45;&gt;2308676275168 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>2308676475984-&gt;2308676275168</title>\n",
       "<path d=\"M758.756,-332.366C775.578,-322.634 795.503,-311.106 813.242,-300.842\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"815.388,-303.644 822.291,-295.607 811.882,-297.585 815.388,-303.644\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308677145376&#45;&gt;2308676275168 -->\n",
       "<g class=\"edge\" id=\"edge13\"><title>2308677145376-&gt;2308676275168</title>\n",
       "<path d=\"M970.965,-332.366C952.45,-322.5 930.473,-310.788 911.015,-300.419\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"912.456,-297.221 901.984,-295.607 909.164,-303.399 912.456,-297.221\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308677904872 -->\n",
       "<g class=\"node\" id=\"node15\"><title>2308677904872</title>\n",
       "<polygon fill=\"none\" points=\"1106.5,-249.5 1106.5,-295.5 1552.5,-295.5 1552.5,-249.5 1106.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1218\" y=\"-268.8\">tag_initial_state_concat: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"1329.5,-249.5 1329.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1357.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1329.5,-272.5 1385.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1357.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1385.5,-249.5 1385.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1469\" y=\"-280.3\">[(None, 128), (None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"1385.5,-272.5 1552.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1469\" y=\"-257.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 2309531317864&#45;&gt;2308677904872 -->\n",
       "<g class=\"edge\" id=\"edge14\"><title>2309531317864-&gt;2308677904872</title>\n",
       "<path d=\"M1311.33,-332.366C1313.89,-324.062 1316.86,-314.451 1319.64,-305.434\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1323.07,-306.194 1322.68,-295.607 1316.38,-304.13 1323.07,-306.194\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2309530983616&#45;&gt;2308677904872 -->\n",
       "<g class=\"edge\" id=\"edge15\"><title>2309530983616-&gt;2308677904872</title>\n",
       "<path d=\"M1514.43,-332.473C1481.67,-322.027 1442.37,-309.494 1408.44,-298.673\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1409.15,-295.226 1398.56,-295.522 1407.02,-301.895 1409.15,-295.226\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308638833072 -->\n",
       "<g class=\"node\" id=\"node16\"><title>2308638833072</title>\n",
       "<polygon fill=\"none\" points=\"228,-166.5 228,-212.5 627,-212.5 627,-166.5 228,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"358\" y=\"-185.8\">batch_normalization_1: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"488,-166.5 488,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"516\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"488,-189.5 544,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"516\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"544,-166.5 544,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"585.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"544,-189.5 627,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"585.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 2308586148136&#45;&gt;2308638833072 -->\n",
       "<g class=\"edge\" id=\"edge16\"><title>2308586148136-&gt;2308638833072</title>\n",
       "<path d=\"M415.872,-249.366C417.512,-241.062 419.411,-231.451 421.192,-222.434\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"424.629,-223.096 423.133,-212.607 417.762,-221.739 424.629,-223.096\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308678544296 -->\n",
       "<g class=\"node\" id=\"node17\"><title>2308678544296</title>\n",
       "<polygon fill=\"none\" points=\"661,-166.5 661,-212.5 1060,-212.5 1060,-166.5 661,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"791\" y=\"-185.8\">batch_normalization_2: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"921,-166.5 921,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"949\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"921,-189.5 977,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"949\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"977,-166.5 977,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1018.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"977,-189.5 1060,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1018.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 2308676275168&#45;&gt;2308678544296 -->\n",
       "<g class=\"edge\" id=\"edge17\"><title>2308676275168-&gt;2308678544296</title>\n",
       "<path d=\"M860.5,-249.366C860.5,-241.152 860.5,-231.658 860.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"864,-222.607 860.5,-212.607 857,-222.607 864,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2309533067640 -->\n",
       "<g class=\"node\" id=\"node18\"><title>2309533067640</title>\n",
       "<polygon fill=\"none\" points=\"1104,-166.5 1104,-212.5 1503,-212.5 1503,-166.5 1104,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1234\" y=\"-185.8\">batch_normalization_3: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"1364,-166.5 1364,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1392\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1364,-189.5 1420,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1392\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1420,-166.5 1420,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1461.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"1420,-189.5 1503,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1461.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 2308677904872&#45;&gt;2309533067640 -->\n",
       "<g class=\"edge\" id=\"edge18\"><title>2308677904872-&gt;2309533067640</title>\n",
       "<path d=\"M1322.4,-249.366C1319.7,-240.973 1316.58,-231.245 1313.66,-222.143\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1316.98,-221.059 1310.6,-212.607 1310.32,-223.198 1316.98,-221.059\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2309532963168 -->\n",
       "<g class=\"node\" id=\"node19\"><title>2309532963168</title>\n",
       "<polygon fill=\"none\" points=\"605.5,-83.5 605.5,-129.5 1115.5,-129.5 1115.5,-83.5 605.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"712\" y=\"-102.8\">concate_three_vector: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"818.5,-83.5 818.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"846.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"818.5,-106.5 874.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"846.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"874.5,-83.5 874.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"995\" y=\"-114.3\">[(None, 256), (None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"874.5,-106.5 1115.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"995\" y=\"-91.3\">(None, 768)</text>\n",
       "</g>\n",
       "<!-- 2308638833072&#45;&gt;2309532963168 -->\n",
       "<g class=\"edge\" id=\"edge19\"><title>2308638833072-&gt;2309532963168</title>\n",
       "<path d=\"M545.247,-166.473C603.247,-155.624 673.281,-142.522 732.594,-131.427\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"733.59,-134.801 742.776,-129.522 732.303,-127.921 733.59,-134.801\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2308678544296&#45;&gt;2309532963168 -->\n",
       "<g class=\"edge\" id=\"edge20\"><title>2308678544296-&gt;2309532963168</title>\n",
       "<path d=\"M860.5,-166.366C860.5,-158.152 860.5,-148.658 860.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"864,-139.607 860.5,-129.607 857,-139.607 864,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2309533067640&#45;&gt;2309532963168 -->\n",
       "<g class=\"edge\" id=\"edge21\"><title>2309533067640-&gt;2309532963168</title>\n",
       "<path d=\"M1183.03,-166.473C1123.57,-155.601 1051.75,-142.468 990.984,-131.358\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"991.409,-127.878 980.943,-129.522 990.15,-134.764 991.409,-127.878\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2309535675896 -->\n",
       "<g class=\"node\" id=\"node20\"><title>2309535675896</title>\n",
       "<polygon fill=\"none\" points=\"732,-0.5 732,-46.5 989,-46.5 989,-0.5 732,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"784\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"836,-0.5 836,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"864\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"836,-23.5 892,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"864\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"892,-0.5 892,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"940.5\" y=\"-31.3\">(None, 768)</text>\n",
       "<polyline fill=\"none\" points=\"892,-23.5 989,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"940.5\" y=\"-8.3\">(None, 29160)</text>\n",
       "</g>\n",
       "<!-- 2309532963168&#45;&gt;2309535675896 -->\n",
       "<g class=\"edge\" id=\"edge22\"><title>2309532963168-&gt;2309535675896</title>\n",
       "<path d=\"M860.5,-83.3664C860.5,-75.1516 860.5,-65.6579 860.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"864,-56.6068 860.5,-46.6068 857,-56.6069 864,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "SVG(model_to_dot(tag_identifier_1,show_shapes=True).create(prog=\"dot\",format=\"svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 142030 samples, validate on 60870 samples\n",
      "Epoch 1/20\n",
      "142030/142030 [==============================] - 4201s 30ms/step - loss: 36.5706 - acc: 0.2444 - val_loss: 41.8571 - val_acc: 0.3159\n",
      "Epoch 2/20\n",
      "142030/142030 [==============================] - 4317s 30ms/step - loss: 33.6418 - acc: 0.2922 - val_loss: 41.2970 - val_acc: 0.2692\n",
      "Epoch 3/20\n",
      "142030/142030 [==============================] - 4293s 30ms/step - loss: 32.4478 - acc: 0.3027 - val_loss: 35.4981 - val_acc: 0.2616\n",
      "Epoch 4/20\n",
      "142030/142030 [==============================] - 4297s 30ms/step - loss: 31.7617 - acc: 0.3293 - val_loss: nan - val_acc: 0.3153\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\callbacks.py:543: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142030/142030 [==============================] - 4304s 30ms/step - loss: nan - acc: 0.1276 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "130368/142030 [==========================>...] - ETA: 5:40 - loss: nan - acc: 1.5341e-05"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-eb1e1ea00852>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m            ([X_test_title,X_test_songs,X_test_tags],\n\u001b[0;32m      7\u001b[0m             y_test),\n\u001b[1;32m----> 8\u001b[1;33m            callbacks = [early_stopping])\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tag_identifier_1.fit([X_train_title,X_train_songs,X_train_tags]\n",
    "           ,y_train,\n",
    "           epochs=20,\n",
    "           batch_size = 64,\n",
    "           validation_data = \n",
    "           ([X_test_title,X_test_songs,X_test_tags],\n",
    "            y_test),\n",
    "           callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1_shape = X_train_title.shape[1]\n",
    "inputs_2_shape = X_train_songs.shape[1]\n",
    "inputs_3_shape = X_train_tags.shape[1]\n",
    "\n",
    "song_embedding_shape = len(dictionary_songs)\n",
    "tag_embedding_shape = len(dictionary)\n",
    "\n",
    "output_shape = tags_output.shape[1]\n",
    "#tf.reset_default_graph\n",
    "\n",
    "## GRU 입력 전 사전처리 모듈(임베딩 -> 컨볼루션 -> 맥스풀링)\n",
    "\n",
    "inputs_1 = layers.Input(shape=[inputs_1_shape],name = \"Feed_Sentence\")\n",
    "\n",
    "embed = layers.Embedding(len(word2idx),128)\n",
    "\n",
    "embed_i = embed(inputs_1)\n",
    "            \n",
    "cnns = layers.Conv1D(128,3,padding=\"valid\",activation=\"relu\",strides=1)(embed_i)\n",
    "cnns = layers.BatchNormalization(axis = 1)(cnns)\n",
    "cnns = layers.Dropout(0.2)(cnns)\n",
    "cnns = layers.MaxPooling1D(pool_size = 4)(cnns)\n",
    "\n",
    "\n",
    "\n",
    "#Bi-GRU 인코더 - 디코더 네트워크\n",
    "Encoder1 = layers.GRU(128,return_sequences = True,return_state = True,input_shape=(None,128),name=\"Encoder1\")\n",
    "Encoder2 = layers.GRU(128,return_sequences = True,return_state = True,go_backwards = True,input_shape=(None,128),name=\"Encoder2\")\n",
    "attention_matrix1,initial_1 = Encoder1(cnns)\n",
    "attention_matrix2,initial_2 = Encoder2(cnns)\n",
    "\n",
    "attention_matrix = layers.Concatenate(axis=-1,name = \"attention_matrix\")([attention_matrix1,attention_matrix2])\n",
    "\n",
    "attention_matrix = layers.BatchNormalization(axis = 1)(attention_matrix)\n",
    "\n",
    "\n",
    "Decoder1 = layers.GRU(128,return_sequences = True,input_shape=(None,128),name=\"Decoder1\")\n",
    "Decoder2 =layers.GRU(128,return_sequences = True,input_shape=(None,128),name=\"Decoder2\")\n",
    "Decoder1_output = Decoder1(cnns,initial_state = initial_1)\n",
    "Decoder2_output = Decoder2(cnns,initial_state = initial_2)\n",
    "\n",
    "Decoder_output = layers.Concatenate(axis=-1,name=\"Decoder_output\")([Decoder1_output,Decoder2_output])\n",
    "\n",
    "Decoder_output = layers.BatchNormalization(axis = 1)(Decoder_output)\n",
    "\n",
    "##어텐션 메커니즘 부분\n",
    "\n",
    "#normalize = True로 켠 상태에서, dot product 유사도를 구할 수 있도록 둘을 내적한다.  \n",
    "dot_product_similarity = layers.dot([Decoder_output,attention_matrix],axes = -1,normalize=True,name=\"dot_product_similarity\")\n",
    "\n",
    "#유사도 벡터를 softmax층에 통과시켜 총합이 1인 확률로 변환한다. 이를 attention_score로 명명한다.\n",
    "attention_score_layer = layers.Softmax(axis=-1,name=\"attention_score_from_Softmax\") \n",
    "attention_score = attention_score_layer(dot_product_similarity)\n",
    "\n",
    "#Softmax 변환된 attention_score를 최초의 attention_matrix와 각각 내적한다.\n",
    "context_vector = layers.Lambda(lambda x: K.tf.matmul(K.permute_dimensions(x[1],pattern=(0,2,1)),x[0]),\n",
    "                                          name=\"weighted_and_making_context_vector\")([attention_score,attention_matrix])\n",
    "context_vector = layers.Permute((2,1))(context_vector)\n",
    "\n",
    "# 하나로  합쳐서  가중치를  모든  Context Vector들이  공유하게  한다..\n",
    "context_vector = layers.Lambda(lambda x: K.sum(x, axis=1),name=\"add_context_vectors_to_one\")(context_vector)\n",
    "\n",
    "\n",
    "\n",
    "inputs_2 = layers.Input(shape = [inputs_2_shape],name = \"songs_input\")\n",
    "\n",
    "#곡에 대한 임베딩  레이어를 설정한다.\n",
    "\n",
    "embed_2 = layers.Embedding(input_dim = song_embedding_shape + 1 , output_dim = 128, mask_zero = True)\n",
    "\n",
    "embed_song = embed_2(inputs_2)\n",
    "\n",
    "#임베딩 조회를 마치고 나온 임베딩 벡터들의 관계를 학습한다.\n",
    "\n",
    "#song_dense = layers.Dense(256,activation = \"relu\")(embed_song)\n",
    "embed_song = layers.Dropout(0.2)(embed_song)\n",
    "\n",
    "embed_song = layers.BatchNormalization(axis = 1)(embed_song)\n",
    "\n",
    "Encoder3 = layers.GRU(128,input_shape=(None,128),name=\"Encoder3\")\n",
    "Encoder4 = layers.GRU(128,go_backwards = True,input_shape=(None,128),name=\"Encoder4\")\n",
    "\n",
    "initial_3 = Encoder3(embed_song)\n",
    "initial_4 = Encoder4(embed_song)\n",
    "\n",
    "song_initial_concat = layers.Concatenate(axis = -1,name=\"song_initial_state_concat\")([initial_3,initial_4])\n",
    "\n",
    "song_dense = layers.BatchNormalization(axis = 1)(song_initial_concat)\n",
    "\n",
    "\n",
    "inputs_3 = layers.Input(shape = [inputs_3_shape],name = \"tags_input\")\n",
    "\n",
    "# 태그에 대한 임베딩 레이어를 설정한다.\n",
    "\n",
    "embed_3 = layers.Embedding(tag_embedding_shape+1,128,mask_zero=True)\n",
    "\n",
    "embed_tag = embed_3(inputs_3)\n",
    "\n",
    "#임베딩 조회를 마치고 나온 임베딩 벡터들의 관계를 학습한다.\n",
    "\n",
    "#tag_dense = layers.Dense(256,activation = \"relu\")(embed_tag)\n",
    "embed_tag = layers.Dropout(0.2)(embed_tag)\n",
    "\n",
    "embed_tag = layers.BatchNormalization(axis = 1)(embed_tag)\n",
    "\n",
    "Encoder5 = layers.GRU(128,input_shape=(None,128),name=\"Encoder5\")\n",
    "Encoder6 = layers.GRU(128,go_backwards = True,input_shape=(None,128),name=\"Encoder6\")\n",
    "\n",
    "initial_5 = Encoder5(embed_tag)\n",
    "initial_6 = Encoder6(embed_tag)\n",
    "\n",
    "tag_initial_concat = layers.Concatenate(axis = -1,name=\"tag_initial_state_concat\")([initial_5,initial_6])\n",
    "\n",
    "tag_dense = layers.BatchNormalization(axis = 1)(tag_initial_concat)\n",
    "\n",
    "\n",
    "\n",
    "# 두 벡터를  concat 하는  방안과,, add 하는  방안  두개를  모두  검토하자..\n",
    "## concat : 두  Input Vector의  가중치가  분리된다..\n",
    "## add : 두  Input Vector가  동일한  가중치를  공유한다..\n",
    "\n",
    "concat_vector = layers.Concatenate(axis = -1, name = \"concate_three_vector\")([context_vector,\n",
    "                                                                              song_dense,\n",
    "                                                                                tag_dense\n",
    "                                                                             ])\n",
    "\n",
    "predicts = layers.Dense(output_shape,activation=\"relu\")(concat_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_identifier_2.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 142030 samples, validate on 60870 samples\n",
      "Epoch 1/20\n",
      " 11456/142030 [=>............................] - ETA: 1:59:34 - loss: 42.8366 - acc: 0.1501"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-4c766cad7bbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m            ([X_test_title,X_test_songs,X_test_tags],\n\u001b[0;32m      7\u001b[0m             y_test),\n\u001b[1;32m----> 8\u001b[1;33m            callbacks = [early_stopping])\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\envs\\tensorflow36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tag_identifier_2.fit([X_train_title,X_train_songs,X_train_tags]\n",
    "           ,y_train,\n",
    "           epochs=20,\n",
    "           batch_size = 64,\n",
    "           validation_data = \n",
    "           ([X_test_title,X_test_songs,X_test_tags],\n",
    "            y_test),\n",
    "           callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해야할 일들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) like_cnt와 tag의 워드 임베딩간  동질성은 상관관계가 있다.\n",
    "### 2) like_cnt를 하이퍼 파라미터로 사용하는 그리드 서치의 작성\n",
    "### 3) tag_input(INPUT_3)을 기존의 매니-핫-인코딩이  아닌 워드  임베딩상의  중심점을 활용\n",
    "#### (1) tag가 10개 있다고 칠 때, 10 개의  Vector를 모두  더해서  n으로 나눈다.\n",
    "\n",
    "### 4) 태그 예측시\n",
    "#### (1) 태그 절반 + 노래 이름\n",
    "#### (2) 오직 노래 이름\n",
    "#### (3) 일단은  (2)부터 진행\n",
    "\n",
    "### 5) 노래 예측시\n",
    "#### (1) 노래 절반 + 태그\n",
    "#### (2) 오직  태그만\n",
    "#### (3) 일단은  (2)부터 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원본 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1_shape = None\n",
    "inputs_2_shape = 20202\n",
    "vocabulary = np.array([i for i in range(0,1000)])\n",
    "tag_num = 29610\n",
    "#tf.reset_default_graph\n",
    "\n",
    "## GRU 입력 전 사전처리 모듈(임베딩 -> 컨볼루션 -> 맥스풀링)\n",
    "\n",
    "inputs_1 = layers.Input(shape=[inputs_1_shape],name = \"Feed_Sentence\")\n",
    "\n",
    "embed = layers.Embedding(len(vocabulary)+1,128)\n",
    "\n",
    "embed_i = embed(inputs_1)\n",
    "            \n",
    "cnns = layers.Conv1D(256,3,padding=\"valid\",activation=\"relu\",strides=1)(embed_i)\n",
    "cnns = layers.Dropout(0.2)(cnns)\n",
    "cnns = layers.MaxPooling1D(pool_size = 4)(cnns)\n",
    "\n",
    "\n",
    "#Bi-GRU 인코더 - 디코더 네트워크\n",
    "Encoder1 = layers.GRU(128,return_sequences = True,return_state = True,input_shape=(None,128),name=\"Encoder1\")\n",
    "Encoder2 = layers.GRU(128,return_sequences = True,return_state = True,go_backwards = True,input_shape=(None,128),name=\"Encoder2\")\n",
    "attention_matrix1,initial_1 = Encoder1(cnns)\n",
    "attention_matrix2,initial_2 = Encoder2(cnns)\n",
    "attention_matrix = layers.Concatenate(axis=-1,name = \"attention_matrix\")([attention_matrix1,attention_matrix2])\n",
    "\n",
    "\n",
    "Decoder1 = layers.GRU(128,return_sequences = True,input_shape=(None,128),name=\"Decoder1\")\n",
    "Decoder2 =layers.GRU(128,return_sequences = True,input_shape=(None,128),name=\"Decoder2\")\n",
    "Decoder1_output = Decoder1(cnns,initial_state = initial_1)\n",
    "Decoder2_output = Decoder2(cnns,initial_state = initial_2)\n",
    "\n",
    "Decoder_output = layers.Concatenate(axis=-1,name=\"Decoder_output\")([Decoder1_output,Decoder2_output])\n",
    "\n",
    "\n",
    "##어텐션 메커니즘 부분\n",
    "\n",
    "#normalize = True로 켠 상태에서, dot product 유사도를 구할 수 있도록 둘을 내적한다.  \n",
    "dot_product_similarity = layers.dot([Decoder_output,attention_matrix],axes = -1,normalize=True,name=\"dot_product_similarity\")\n",
    "\n",
    "#유사도 벡터를 softmax층에 통과시켜 총합이 1인 확률로 변환한다. 이를 attention_score로 명명한다.\n",
    "attention_score_layer = layers.Softmax(axis=-1,name=\"attention_score_from_Softmax\") \n",
    "attention_score = attention_score_layer(dot_product_similarity)\n",
    "\n",
    "#Softmax 변환된 attention_score를 최초의 attention_matrix와 각각 내적한다.\n",
    "context_vector = layers.Lambda(lambda x: K.tf.matmul(K.permute_dimensions(x[1],pattern=(0,2,1)),x[0]),\n",
    "                                          name=\"weighted_and_making_context_vector\")([attention_score,attention_matrix])\n",
    "context_vector = layers.Permute((2,1))(context_vector)\n",
    "\n",
    "#_,context_vector = layers.GRU(64,return_state = True,name=\"Hierarchical_GRU\")(weighted_attention_matrix)\n",
    "\n",
    "#concat = layers.Concatenate(axis=-1,name = \"Concatenate_Decoder_O_and_Context_Vector\")([Decoder_output,weighted_attention_matrix])\n",
    "\n",
    "# 하나로  합쳐서  가중치를  모든  Context Vector들이  공유하게  한다..\n",
    "context_vector = layers.Lambda(lambda x: K.sum(x, axis=1),name=\"add_context_vectors_to_one\")(context_vector)\n",
    "\n",
    "#Feed_forward = layers.Dense(512,activation = \"tanh\",name=\"Feed_forward\")\n",
    "#finally_output = Feed_forward(context_vector)\n",
    "\n",
    "inputs_2 = layers.Input([inputs_2_shape])\n",
    "song_dense = layers.Dense(256,activation = \"relu\")(inputs_2)\n",
    "song_dense = layers.Dropout(0.2)(song_dense)\n",
    "#song_dense_2 = layers.Dense(32, activation = \"relu\")(song_dense)\n",
    "\n",
    "# 두 벡터를  concat 하는  방안과,, add 하는  방안  두개를  모두  검토하자..\n",
    "## concat : 두  Input Vector의  가중치가  분리된다..\n",
    "## add : 두  Input Vector가  동일한  가중치를  공유한다..\n",
    "concat_two_vector = layers.Concatenate(axis = -1, name = \"concate_two_vector\")([context_vector,song_dense])\n",
    "\n",
    "#다중 라벨 분류를 위해선 각 정답 노드들의 활성화 함수를 sigmoid로 잡는다. \n",
    "\n",
    "predicts = layers.Dense(tag_num,activation=\"sigmoid\")(concat_two_vector)\n",
    "\n",
    "GRUs_2 = Model(inputs = [inputs_1,inputs_2], outputs = [predicts])\n",
    "GRUs_2.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor36",
   "language": "python",
   "name": "tensorflow36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
